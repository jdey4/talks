<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <title>Learning</title>
  <meta name="Generator" content="Cocoa HTML Writer">
  <meta name="CocoaVersion" content="1894.6">
  <style type="text/css">
    p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; line-height: 18.0px; font: 12.0px Menlo; color: #4689cc; -webkit-text-stroke: #4689cc}
    p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; line-height: 18.0px; font: 12.0px Menlo; color: #cacaca; -webkit-text-stroke: #cacaca; min-height: 14.0px}
    p.p3 {margin: 0.0px 0.0px 0.0px 0.0px; line-height: 18.0px; font: 12.0px Menlo; color: #cacaca; -webkit-text-stroke: #cacaca}
    p.p4 {margin: 0.0px 0.0px 0.0px 0.0px; line-height: 18.0px; font: 12.0px Menlo; color: #c27e65; -webkit-text-stroke: #c27e65}
    p.p5 {margin: 0.0px 0.0px 0.0px 0.0px; line-height: 18.0px; font: 12.0px Menlo; color: #598a43; -webkit-text-stroke: #598a43}
    p.p6 {margin: 0.0px 0.0px 0.0px 0.0px; line-height: 18.0px; font: 12.0px Menlo; color: #ee2e38; -webkit-text-stroke: #ee2e38}
    p.p7 {margin: 0.0px 0.0px 0.0px 0.0px; line-height: 18.0px; font: 12.0px Menlo; color: #d4d69a; -webkit-text-stroke: #d4d69a}
    p.p8 {margin: 0.0px 0.0px 0.0px 0.0px; line-height: 18.0px; font: 12.0px Menlo; color: #8cd3fe; -webkit-text-stroke: #8cd3fe}
    span.s1 {font-kerning: none; color: #6d6d6d; background-color: #171717; -webkit-text-stroke: 0px #6d6d6d}
    span.s2 {font-kerning: none; background-color: #171717}
    span.s3 {font-kerning: none; color: #cacaca; background-color: #171717; -webkit-text-stroke: 0px #cacaca}
    span.s4 {font-kerning: none; color: #8cd3fe; background-color: #171717; -webkit-text-stroke: 0px #8cd3fe}
    span.s5 {font-kerning: none}
    span.s6 {font-kerning: none; color: #4689cc; background-color: #171717; -webkit-text-stroke: 0px #4689cc}
    span.s7 {font-kerning: none; color: #c27e65; background-color: #171717; -webkit-text-stroke: 0px #c27e65}
    span.s8 {font-kerning: none; color: #ee2e38; background-color: #171717; -webkit-text-stroke: 0px #ee2e38}
    span.s9 {font-kerning: none; color: #cdad6a; background-color: #171717; -webkit-text-stroke: 0px #cdad6a}
    span.s10 {font-kerning: none; color: #d4d69a; background-color: #171717; -webkit-text-stroke: 0px #d4d69a}
    span.s11 {font-kerning: none; color: #b76fb3; background-color: #171717; -webkit-text-stroke: 0px #b76fb3}
    span.s12 {font-kerning: none; color: #598a43; background-color: #171717; -webkit-text-stroke: 0px #598a43}
  </style>
</head>
<body>
<p class="p1"><span class="s1">&lt;!</span><span class="s2">DOCTYPE</span><span class="s3"> </span><span class="s4">html</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;</span><span class="s2">html</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p1"><span class="s1">&lt;</span><span class="s2">head</span><span class="s1">&gt;</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">title</span><span class="s1">&gt;</span><span class="s2">Learning</span><span class="s1">&lt;/</span><span class="s6">title</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">meta</span><span class="s3"> </span><span class="s4">http-equiv</span><span class="s3">=</span><span class="s2">"Content-Type"</span><span class="s3"> </span><span class="s4">content</span><span class="s3">=</span><span class="s2">"text/html; charset=UTF-8"</span><span class="s3"> </span><span class="s1">/&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">link</span><span class="s3"> </span><span class="s4">rel</span><span class="s3">=</span><span class="s2">"stylesheet"</span><span class="s3"> </span><span class="s4">href</span><span class="s3">=</span><span class="s2">"fonts/quadon/quadon.css"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">link</span><span class="s3"> </span><span class="s4">rel</span><span class="s3">=</span><span class="s2">"stylesheet"</span><span class="s3"> </span><span class="s4">href</span><span class="s3">=</span><span class="s2">"fonts/gentona/gentona.css"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">link</span><span class="s3"> </span><span class="s4">rel</span><span class="s3">=</span><span class="s2">"stylesheet"</span><span class="s3"> </span><span class="s4">href</span><span class="s3">=</span><span class="s2">"slides_style_i.css"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">script</span><span class="s3"> </span><span class="s4">type</span><span class="s3">=</span><span class="s2">"text/javascript"</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"assets/plotly/plotly-latest.min.js"</span><span class="s1">&gt;&lt;/</span><span class="s6">script</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">head</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p1"><span class="s1">&lt;</span><span class="s2">body</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s2">textarea</span><span class="s3"> </span><span class="s4">id</span><span class="s3">=</span><span class="s7">"source"</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Lifelong Learning: Theory and Practice</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">PI: Joshua T. Vogelstein, [JHU](https://www.jhu.edu/) </span><span class="s1">&lt;</span><span class="s6">br</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">Jayanta Dey, Will LeVine, Hayden Helm, Ronak Mehta,</span></p>
<p class="p3"><span class="s2">Carey E. Priebe<span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2">&lt;!-- | Joshua T. Vogelstein &lt;br&gt; --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- [Microsoft Research](https://www.microsoft.com/en-us/research/): Weiwei Yang | Jonathan Larson | Bryan Tower | Chris White --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 40%](images/neurodata_blue.png)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">Biological agents progressively build representations to transfer both forward &amp; backward</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p6"><span class="s2">&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span><span class="s3"> HEAD</span></p>
<p class="p3"><span class="s2">![:scale 250%](images/learning_schema_darpa.png)</span></p>
<p class="p3"><span class="s2">=======</span></p>
<p class="p3"><span class="s2">![:scale 110%](images/learning_schema_darpa.png)</span></p>
<p class="p3"><span class="s2">&gt;&gt;&gt;&gt;&gt;&gt;&gt; c541745bae4ad3f2aeab1ca5624e4ba92f77bc08</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p3"><span class="s2">class:middle</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">| Biology Learning | Machine Learning |</span></p>
<p class="p3"><span class="s2">| :--- <span class="Apple-converted-space">  </span>| :---<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">| old | new<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">| little | big<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">| light | heavy<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">| free | expensive<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">| imprecise | precise</span></p>
<p class="p3"><span class="s2">| energy efficient | hog<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">| data efficient | glutton<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">| remembers | usually forgets<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">| builds models | sometimes</span></p>
<p class="p3"><span class="s2">| .r[extrapolates] | interpolates<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Motivating questions</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">1. Are biological and machine learning trying to do the same thing?</span></p>
<p class="p3"><span class="s2">2. Do they use the same algorithms? Could they?<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">2. Can we<span class="Apple-converted-space">  </span>talk about them using the same terminology?</span></p>
<p class="p3"><span class="s2">3. Can we characterize their abilities using the same units?<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">--</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">They are both about .ye[learning], so....should they?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### What is learning?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">--</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p1"><span class="s1">&lt;</span><span class="s2">br</span><span class="s1">&gt;</span></p>
<p class="p3"><span class="s2">"The acquisition of knowledge or skills through experience, study, or by being taught."</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">-- Google, 2020</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">--</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">"A computer .ye[program] is set to learn from an .ye[experience] E with respect to some .ye[task] T and some .ye[performance measure] P if its performance on T as measured by P .ye[improves] with experience E."</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">-- Tom Mitchell, 1997</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">--</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">".ye[$f$] learns from .ye[data] $\mathbf{Z}_n$ w.r.t. .ye[task] $t$<span class="Apple-converted-space">  </span>when its .ye[performance] at $t$ improves due to $\mathbf{Z}_n$."</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">-- jovo, 2020</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- \mathbb{E}\left[\frac{R(f(\bold{Z}_n))}{R(f(\bold{Z}_0))}\right] = \frac{\mathbb{E}[R(f(\bold{Z}_n))]}{R(f(\bold{Z}_0))}<span class="Apple-converted-space">  </span>--&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### What are the data?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">The data are determined by physical implementation of the system:</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">-<span class="Apple-converted-space">  </span>.ye[Measurement space]: $\mathcal{Z}$,<span class="Apple-converted-space">  </span>determined by available sensors</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- visual, auditory, tactile, text, vectors, networks, etc.</span></p>
<p class="p3"><span class="s2">- .ye[Action space]: $\mathcal{A}$,<span class="Apple-converted-space">  </span>determined by available actuators</span></p>
<p class="p1"><span class="s3"><span class="Apple-converted-space">  </span>-<span class="Apple-converted-space">  </span></span><span class="s2">&amp;rarr;</span><span class="s3">, </span><span class="s2">&amp;larr;</span><span class="s3">, </span><span class="s2">&amp;uarr;</span><span class="s3">, </span><span class="s2">&amp;darr;</span><span class="s3">, etc.</span></p>
<p class="p5"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s2">&lt;!-- , {reject, fail to reject}, $\mathbb{R}$ --&gt;</span></p>
<p class="p3"><span class="s2">- .ye[Query space]: $\mathcal{Q}$, determined by system's "interface"</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- in which cluster is $z$? what is this object? etc.</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space"> </span>Classification Example</span></p>
<p class="p3"><span class="s2">- $z_i = (x_i,y_i)$ where $\mathcal{X}=\mathbb{R}^p$ and $\mathcal{Y}=\lbrace 0,1\rbrace$<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- $a_i \in \lbrace 0, 1 \rbrace = \mathcal{Y}$ are class labels<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- $q_i \in \mathcal{X}=\mathbb{R}^p$ are possible feature vectors with unknown class labels<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- TODO we play fast and loose with whether the dataset is in \mathcal{D} vs \mathcal{Z}. i'd like to be consistent --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- TODO in XOR experiments, replace purple with orange --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- TODO@jovo<span class="Apple-converted-space">  </span>the supertask learning slides are too complex, i'll simplify dramatically --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- TODO@jovo move some supertask slides to appendix --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- TODO@jovo reorganize compositional hypothesis slides --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- TODO@jovo more about internal models, generalization, motivation, etc. --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- TODO@jovo 2nd what is learning slide uses old notation --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- TODO@jovo maybe update xor/nxor figure to show forgetting instead of RF? --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### What is $f$?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">We get to choose the learning algorithm $f$:</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- $\vec{\mathcal{Z}} = \bigcup_{n = 0}^{\infty}\mathcal{Z}^n$, a .ye[data corpus] $\mathbf{Z}_n= \lbrace Z_1, Z_2, \ldots, Z_n \rbrace$, and $\mathcal{Z}^0 = \emptyset$ is the empty set,</span></p>
<p class="p3"><span class="s2">meaning no data</span></p>
<p class="p3"><span class="s2">-<span class="Apple-converted-space">  </span>$\mathcal{H} = \lbrace h : \mathcal{Q} \rightarrow \mathcal{A} \rbrace$,<span class="Apple-converted-space">  </span>where a .ye[hypothesis] $h$ takes an action on the basis of a query</span></p>
<p class="p3"><span class="s2">- $\Theta$ the parameter space, which could be priors, inductive bias of $\mathcal{H}$, estimation bias of $f$, pre-training, etc. or any combination thereof</span></p>
<p class="p3"><span class="s2">- $f: \vec{\mathcal{Z}} \times \Theta \to \mathcal{H}$, a learning .ye[algorithm] with parameter $\theta_0$ maps data sets from $\mathcal{Z}$ to a hypothesis<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- For notational convenience, we will suppress the $\theta_0$, as it is always present</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Supervised machine learning example</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- $\mathbf{Z}_n = (X_1, Y_1),<span class="Apple-converted-space">  </span>\ldots, (X_n, Y_n)$</span></p>
<p class="p3"><span class="s2">- $h$ is *RandomForestClassifier.predict*</span></p>
<p class="p3"><span class="s2">- $f$ is *RandomForestClassifier.fit*</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### What is performance?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- We care about future performance on unseen queries, not past performance</span></p>
<p class="p3"><span class="s2">- To make any out-of-data claims requires .ye[assumptions]</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- A .ye[datum] $Z\_i$ is sampled iid from some true but unknown distribution $P\_{Z} \in \,</span></p>
<p class="p3"><span class="s2">\mathcal{P}$</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- A .ye[query], $q \in \mathcal{Q}$ is sampled iid from some true but unknown distribution $P_Q \in \, \mathcal{P}_Q$<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- An optimal .ye[action], $a \in \mathcal{A}$ given $q$, is sampled iid from some true but unknown distribution $P\_{A \mid Q} \in \, \mathcal{P}_{A \mid Q}$<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- Let $P\ = P\_{Z} \otimes P\_{A | Q} \otimes P\_Q \in \, \mathcal{P}$ denote the joint .ye[distribution] over samples, queries, and optimal actions</span></p>
<p class="p3"><span class="s2">- $\mathcal{P}$ is called the .ye[statistical model]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### What is performance?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- .ye[Loss],<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>e.g., 0-1 loss: $ \ell(a, a') := \mathbb{I}[a \neq a'] $</span></p>
<p class="p3"><span class="s2">- .ye[Risk]<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>e.g., expected loss: $ R(h) :=<span class="Apple-converted-space">  </span>\ \mathbb{E}\_{Q, A}[\ell(h(Q), A)] $<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- For an algorithm, the risk is $ R(f(\bold{Z}_n)) $</span></p>
<p class="p3"><span class="s2">- .ye[Performance] is defined as expected risk (or .ye[generalization error]):</span></p>
<p class="p3"><span class="s2">$$\mathcal{E}_n(f) := \mathbb{E}_P[R(f(\bold{Z}_n))] $$</span></p>
<p class="p3"><span class="s2">- Performance prior to acquiring data $\mathbf{Z}_n$ is $\mathbb{E}[R(f(Z_0))]$, where $Z_0 = \emptyset$ is the empty set, is a function of $\theta_0$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### What is learning?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">.ye[$f$] learns from .ye[data] $\mathbf{Z}_n$ with respect to .ye[task] $t$<span class="Apple-converted-space">  </span>when its .ye[performance] at $t$ improves due to $\mathbf{Z}_n$,</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">Define .ye[learning efficiency]: $$LE_n(f) := \frac{\mathbb{E}[R(f(Z_0))]}{\mathbb{E}[R(f(\bold{Z}_n))]}$$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p1"><span class="s1">&lt;</span><span class="s2">br</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$f$ learns from $\mathbf{Z}_n$ with respect to task $t$ when $LE_n(f)<span class="Apple-converted-space">  </span>&gt; 1$.</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### What is transfer learning? <span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Also given .ye[side information]</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- other data<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- pre-trained model</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- priors</span></p>
<p class="p3"><span class="s2">- Side information is sampled according to some distribution $P_{Z_0}$, from $\mathcal{Z}$</span></p>
<p class="p3"><span class="s2">- Task data is sampled according to $P_{Z_1}$, also from $\mathcal{Z}$</span></p>
<p class="p3"><span class="s2">- Redefine $\mathbf{Z}_n$ to be the combined data, $\mathbf{Z}_n = \lbrace (Z_i, k_i) \rbrace$, for $i \in [n]$, where $k\_i = j$ if $Z\_i$ is sampled from $P_j$</span></p>
<p class="p3"><span class="s2">- Let $\mathbf{Z}^j_n = \lbrace (Z_i, k_i) \in \mathbf{Z}_n: k_i = j \rbrace$, $i \in [n]$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Learning efficiency is now $LE\_{\mathbf{n}}(f) := \frac{\mathbb{E}[R(f(\mathbf{Z}_n^1))]}{\mathbb{E}[R(f(\mathbf{Z_n}))]}$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$f$ .ye[transfer learns] from $\mathbf{Z}\_{n}^0$ with respect to task $t$ when $LE\_{n}(f) &gt; 1$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- .center[$LE\_{\mathbf{n}}(f) = \frac{\mathbb{E}[R(f(\bold{Z}'\_{\mathbf{n}}))]}{\mathbb{E}[R(f(\mathbf{Z}'\_{0}))]} &gt; 1$<span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2">.]</span></p>
<p class="p5"><span class="s2">--&gt;</span><span class="s3"><span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### What is multitask learning?<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Now we have lots of tasks, say $T$ many, each with their own data $\mathbf{Z}_n^t$, $t \in [T]$, along with some side information $\mathbf{Z}_n^0$</span></p>
<p class="p3"><span class="s2">- Learning efficiency for task $t$: $LE\_{n}^t = \frac{\mathbb{E}[R^t(f(\mathbf{Z}\_{n}^t))]}{\mathbb{E}[R^t(f(\mathbf{Z}\_{n}))]}$</span></p>
<p class="p3"><span class="s2">- Let $n_t = |Z_n^t|$</span></p>
<p class="p3"><span class="s2">- Define .ye[weak] multitask learning efficiency as:</span></p>
<p class="p3"><span class="s2">.center[$LE\_{\mathbf{n}}(f) := \sum\_{t \in [T]}n\_t \cdot LE\_{\mathbf{n}}^t(f)$</span></p>
<p class="p3"><span class="s2">]</span></p>
<p class="p3"><span class="s2">- Define .ye[strong] multitask learning efficiency as:</span></p>
<p class="p3"><span class="s2">.center[$LE\_{\mathbf{n}}(f) := \min\_{t \in [T]} <span class="Apple-converted-space">  </span>LE\_{\mathbf{n}}^t(f)$</span></p>
<p class="p3"><span class="s2">]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$f$ .ye[multitask learns] from<span class="Apple-converted-space">  </span>$\mathbf{Z}\_{\mathbf{n}}$ with respect to tasks $[T]$<span class="Apple-converted-space">  </span>when<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">$LE\_{\mathbf{n}}(f)<span class="Apple-converted-space">  </span>&gt; 1$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p3"><span class="s2">### What is efficient learning?<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Let .ye[$c$] be the upper bound on the .ye[computational cost] of updating $h$ given a new $Z \in \mathcal{Z}$ and choosing an $a \in \mathcal{A}$.</span></p>
<p class="p3"><span class="s2">- Let $\mathcal{F}_e = \lbrace f : \vec{\mathcal{Z}} \to \mathcal{H}$ <span class="Apple-converted-space">  </span>such that<span class="Apple-converted-space">  </span>$f \in poly(n,c) \rbrace$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$f$ .ye[efficiently learns] from $\mathbf{Z}\_n$ with respect to task $t$ when $LE\_n(f) &gt; 1$ and $f \in \mathcal{F}_e$.</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p3"><span class="s2">### What is lifelong learning?<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Now we have a .ye[sequence] of tasks,<span class="Apple-converted-space">  </span>$\mathbf{Z}\_{\mathbf{n}} = [\mathbf{Z}\_{0},<span class="Apple-converted-space">  </span>\mathbf{Z}\_{1}, \ldots, \mathbf{Z}\_{T}]$</span></p>
<p class="p3"><span class="s2">- Learning efficiency for task $t$: $LE\_{\mathbf{n}}^t = \frac{\mathbb{E}[R^t(f(\mathbf{Z}\_{t}))]}{\mathbb{E}[R^t(f(\mathbf{Z}\_{\mathbf{n}}))]}$</span></p>
<p class="p3"><span class="s2">- Define .ye[weak] lifelong learning efficiency as:</span></p>
<p class="p3"><span class="s2">.center[$LE\_{\mathbf{n}}(f) := \sum\_{t \in [T]} LE\_{\mathbf{n}}^t(f)P(t)$</span></p>
<p class="p3"><span class="s2">]</span></p>
<p class="p3"><span class="s2">- Define .ye[strong] lifelong learning efficiency as:</span></p>
<p class="p3"><span class="s2">.center[$LE\_{\mathbf{n}}(f) := \min\_{t \in [T]} <span class="Apple-converted-space">  </span>LE\_{\mathbf{n}}^t(f)$</span></p>
<p class="p3"><span class="s2">]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$f$ .ye[lifelong learns] from<span class="Apple-converted-space">  </span>$\mathbf{Z}\_{\mathbf{n}}$ with respect to tasks $[T]$<span class="Apple-converted-space">  </span>when<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">$LE\_{\mathbf{n}}(f)<span class="Apple-converted-space">  </span>&gt; 1$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p3"><span class="s2">### What is lifelong cheating?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Store every sample you've ever seen<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- Every time we are faced with a new $z$, $q$, or $t$, just update everything in batch mode<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- Now just run your favorite multitask $f$</span></p>
<p class="p3"><span class="s2">- Doing so consumes $\mathcal{O}(n^2)$ resources because $ \sum_{i =1}^n i = n^2$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- So, to differentiate lifelong learning from multitask learning requires a particularly efficient algorithm</span></p>
<p class="p3"><span class="s2">- $f$ must consume less than quadratic resources as a function of $n$,<span class="Apple-converted-space">  </span>$f \in o(n^2,c) $</span></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2">&lt;!-- - Let $\mathcal{F}_{&lt; 2} = \lbrace f : \vec{\mathcal{Z}} \to \mathcal{H}$ <span class="Apple-converted-space">  </span>such that<span class="Apple-converted-space">  </span>$f \in o(n^2,c) \rbrace$ --&gt;</span></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p3"><span class="s2">### What is lifelong learning?<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!--<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">- Now we have a .ye[sequence] of tasks,<span class="Apple-converted-space">  </span>$\mathbf{Z}\_{\mathbf{n}} = [\mathbf{Z}\_{0},<span class="Apple-converted-space">  </span>\mathbf{Z}\_{1}, \ldots, \mathbf{Z}\_{T}]$</span></p>
<p class="p5"><span class="s2">- Learning efficiency for task $t$: $LE\_{\mathbf{n}}^t = \frac{\mathbb{E}[R^t(f(\mathbf{Z}\_{t}))]}{\mathbb{E}[R^t(f(\mathbf{Z}\_{\mathbf{n}}))]}$</span></p>
<p class="p5"><span class="s2">- Define .ye[weak] lifelong learning efficiency as:</span></p>
<p class="p5"><span class="s2">.center[$LE\_{\mathbf{n}}(f) := \sum\_{t \in [T]} LE\_{\mathbf{n}}^t(f)P(t)$</span></p>
<p class="p5"><span class="s2">]</span></p>
<p class="p5"><span class="s2">- Define .ye[strong] lifelong learning efficiency as:</span></p>
<p class="p5"><span class="s2">.center[$LE\_{\mathbf{n}}(f) := \min\_{t \in [T]} <span class="Apple-converted-space">  </span>LE\_{\mathbf{n}}^t(f)$ and<span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2">]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">$f$ .ye[lifelong learns] from<span class="Apple-converted-space">  </span>$\mathbf{Z}\_{\mathbf{n}}$ with respect to tasks $[T]$<span class="Apple-converted-space">  </span>when<span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2">$LE\_{\mathbf{n}}(f)<span class="Apple-converted-space">  </span>&gt; 1$ --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Now we have a .ye[sequence] of tasks,<span class="Apple-converted-space">  </span>$\mathbf{Z}\_{\mathbf{n}} = [\mathbf{Z}\_{0},<span class="Apple-converted-space">  </span>\mathbf{Z}\_{1}, \ldots, \mathbf{Z}\_{T}]$</span></p>
<p class="p3"><span class="s2">- Learning efficiency for task $t$: $LE\_{\mathbf{n}}^t = \frac{\mathbb{E}[R^t(f(\mathbf{Z}\_{t}))]}{\mathbb{E}[R^t(f(\mathbf{Z}\_{\mathbf{n}}))]}$</span></p>
<p class="p3"><span class="s2">- Define .ye[weak] lifelong learning efficiency as:</span></p>
<p class="p3"><span class="s2">.center[$LE\_{\mathbf{n}}(f) := \sum\_{t \in [T]} LE\_{\mathbf{n}}^t(f)P(t)$</span></p>
<p class="p3"><span class="s2">]</span></p>
<p class="p3"><span class="s2">- Define .ye[strong] lifelong learning efficiency as:</span></p>
<p class="p3"><span class="s2">.center[$LE\_{\mathbf{n}}(f) := \min\_{t \in [T]} <span class="Apple-converted-space">  </span>LE\_{\mathbf{n}}^t(f)$</span></p>
<p class="p3"><span class="s2">]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$f$ .ye[lifelong learns] from<span class="Apple-converted-space">  </span>$\mathbf{Z}\_{\mathbf{n}}$ with respect to tasks $[T]$<span class="Apple-converted-space">  </span>when<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">$LE\_{\mathbf{n}}(f)<span class="Apple-converted-space">  </span>&gt; 1$ and $f \in o(n^2,c) $</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!--<span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2">- Now we have a .ye[sequence] of tasks,<span class="Apple-converted-space">  </span>$\mathbf{Z}\_{\mathbf{n}} = [\mathbf{Z}\_{0},<span class="Apple-converted-space">  </span>\mathbf{Z}\_{1}, \ldots, \mathbf{Z}\_{T}]$</span></p>
<p class="p5"><span class="s2">- Learning efficiency for task $t$: $LE\_{\mathbf{n}}^t = \frac{\mathbb{E}[R^t(f(\mathbf{Z}\_{t}))]}{\mathbb{E}[R^t(f(\mathbf{Z}\_{\mathbf{n}}))]}$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">$f$ .w[weakly multitask] learns from<span class="Apple-converted-space">  </span>$\mathbf{Z}\_{\mathbf{n}}$ with respect to tasks $[T]$<span class="Apple-converted-space">  </span>when<span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2">performance improves due to<span class="Apple-converted-space">  </span>other task's data, on .ye[average]<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">.center[$\sum\_{t \in [T]} LE\_{\mathbf{n}}^t(f)P(t)<span class="Apple-converted-space">  </span>&gt; 1$ and $f \in o(n^2,c) $</span></p>
<p class="p5"><span class="s2">]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">$f$ .w[strongly multitask] learns from<span class="Apple-converted-space">  </span>$\mathbf{Z}\_{\mathbf{n}}$ with respect to tasks $[T]$<span class="Apple-converted-space">  </span>when<span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2">performance improves due to other task's data, on .ye[each] task<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">.center[$\prod\_{t \in [T]}<span class="Apple-converted-space">  </span>LE\_{\mathbf{n}}^t(f)<span class="Apple-converted-space">  </span>&gt; 1$ and $f \in \mathcal{F}_{&lt; 2}$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">] --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p3"><span class="s2">### What is forward learning?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Let $n\_t$ be the last occurence of task $t$ in $\mathbf{Z}\_n$<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- Let $\mathbf{Z}\_n^{</span><span class="s8">&lt;</span><span class="s2"> t} = \lbrace Z\_1, Z\_2, \ldots, Z\_{n_t}<span class="Apple-converted-space">  </span>\rbrace$</span></p>
<p class="p3"><span class="s2">- .ye[Forward] learning efficiency is the improvement on task $t$ resulting from all data .ye[preceding] task $t$</span></p>
<p class="p3"><span class="s2">$$<span class="Apple-converted-space">    </span>FLE^t\_{\mathbf{n}}(f) := \frac{\mathbb{E}[R^t(f(\mathbf{Z}^{t}\_n))]}{\mathbb{E}[R^t(f(\mathbf{Z}^{</span><span class="s8">&lt;</span><span class="s2"> t}\_n))]}<span class="Apple-converted-space">  </span>$$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$$<span class="Apple-converted-space">  </span>\text{(weak) } <span class="Apple-converted-space">  </span>FLE^t\_{\mathbf{n}}(f) := \sum\_{t \in [T]} FLE\_{\mathbf{n}}^t(f)P(t) $$</span></p>
<p class="p3"><span class="s2">$$<span class="Apple-converted-space">  </span>\text{(strong) } <span class="Apple-converted-space">  </span>FLE^t\_{\mathbf{n}}(f) := \min\_{t \in [T]} FLE\_{\mathbf{n}}^t(f) $$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p1"><span class="s1">&lt;</span><span class="s2">br</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$f$ .ye[forward learns] if $FLE_{\mathbf{n}}(f) &gt; 1$ and $f \in o(n^2,c)$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p3"><span class="s2">### What is backward learning?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space"> </span>.ye[Backward] learning efficiency<span class="Apple-converted-space">  </span>is the improvement on task $t$ resulting from all data .ye[after] task $t$<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$$<span class="Apple-converted-space">    </span>BLE^t\_{\mathbf{n}}(f) :=<span class="Apple-converted-space">  </span>\frac{\mathbb{E}[R^t(f(\mathbf{Z}^{</span><span class="s8">&lt;</span><span class="s2"> t}\_n))]}{\mathbb{E}[R^t(f(\mathbf{Z}\_n))]}<span class="Apple-converted-space">  </span>$$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$$<span class="Apple-converted-space">  </span>\text{(weak) } <span class="Apple-converted-space">  </span>BLE^t\_{\mathbf{n}}(f) := \sum\_{t \in [T]} BLE\_{\mathbf{n}}^t(f)P(t) $$</span></p>
<p class="p3"><span class="s2">$$<span class="Apple-converted-space">  </span>\text{(strong) } <span class="Apple-converted-space">  </span>BLE^t\_{\mathbf{n}}(f) := \min\_{t \in [T]} BLE\_{\mathbf{n}}^t(f) $$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p1"><span class="s1">&lt;</span><span class="s2">br</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$f$ .ye[backward learns] if $BLE_{\mathbf{n}}(f) &gt; 1$ and $f \in o(n^2,c)$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p3"><span class="s2">### Learning efficiency factorizes</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$$LE\_t(f) := FLE\_t(f)<span class="Apple-converted-space">  </span>\times BLE\_t(f) $$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$$\frac{\mathbb{E}[R^t(f(\mathbf{Z}^t\_n))]}{\mathbb{E}[R^t(f(\mathbf{Z}\_n))]} =<span class="Apple-converted-space">  </span>\frac{\mathbb{E}[R^t(f(\mathbf{Z}^{t}\_n))]}{\mathbb{E}[R^t(f(\mathbf{Z}^{</span><span class="s8">&lt;</span><span class="s2"> t}\_n))]} \times</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">           </span>\frac{\mathbb{E}[R^t(f(\mathbf{Z}^{</span><span class="s8">&lt;</span><span class="s2"> t}\_n))]}{\mathbb{E}[R^t(f(\mathbf{Z}\_n))]}$$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p1"><span class="s1">&lt;</span><span class="s2">br</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">We therefore have a single metric to quantify transfer.</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p3"><span class="s2">### What is progressive learning?<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$f$ weakly/strongly .ye[progressively] learns from<span class="Apple-converted-space">  </span>$\mathbf{Z}\_{\mathbf{n}}$ with respect to tasks $[T]$<span class="Apple-converted-space">  </span>when<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">it weakly/strongly learns both forward and backward<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">.center[$ \min \left(FTE\_{\mathbf{n}}(f),BTE\_{\mathbf{n}}(f) \right)<span class="Apple-converted-space">    </span>&gt; 1<span class="Apple-converted-space">  </span>$ and $f \in o(n^2,c) $</span></p>
<p class="p3"><span class="s2">]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- .center[$\left( FLE\_{\mathbf{n}}^t(f)<span class="Apple-converted-space">  </span>&gt; 1 \right) \times \left( BLE\_{\mathbf{n}}^t(f)<span class="Apple-converted-space">  </span>&gt; 1 \right)<span class="Apple-converted-space">  </span>,<span class="Apple-converted-space">  </span>\ \<span class="Apple-converted-space">  </span>t \in [T]$</span></p>
<p class="p5"><span class="s2">] --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### A taxonomy of<span class="Apple-converted-space">  </span>approaches</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">| Par.<span class="Apple-converted-space">    </span>| </span><span class="s6">&amp;rarr;</span><span class="s2"> | </span><span class="s6">&amp;larr;</span><span class="s2"><span class="Apple-converted-space">  </span>| space <span class="Apple-converted-space">  </span>| time <span class="Apple-converted-space">        </span>| Examples<span class="Apple-converted-space">     </span></span></p>
<p class="p3"><span class="s2">| :---: <span class="Apple-converted-space">  </span>| :---:<span class="Apple-converted-space">  </span>| :---: <span class="Apple-converted-space">  </span>| :---:| :---: <span class="Apple-converted-space">          </span>|<span class="Apple-converted-space">                       </span></span></p>
<p class="p3"><span class="s2">| par <span class="Apple-converted-space">    </span>| +<span class="Apple-converted-space">      </span>| - <span class="Apple-converted-space">      </span>| 1<span class="Apple-converted-space">    </span>| n <span class="Apple-converted-space">              </span>| O-EWC, SI, TL<span class="Apple-converted-space">         </span></span></p>
<p class="p3"><span class="s2">| par <span class="Apple-converted-space">    </span>| +<span class="Apple-converted-space">      </span>| - <span class="Apple-converted-space">      </span>| T<span class="Apple-converted-space">    </span>| n <span class="Apple-converted-space">              </span>| SI <span class="Apple-converted-space">                   </span></span></p>
<p class="p3"><span class="s2">| par <span class="Apple-converted-space">    </span>| +<span class="Apple-converted-space">      </span>| - <span class="Apple-converted-space">      </span>| T<span class="Apple-converted-space">    </span>| nT+T</span><span class="s1">&lt;</span><span class="s6">sup</span><span class="s1">&gt;</span><span class="s2">2</span><span class="s1">&lt;/</span><span class="s6">sup</span><span class="s1">&gt;</span><span class="s2">| EWC<span class="Apple-converted-space">                   </span></span></p>
<p class="p3"><span class="s2">| par <span class="Apple-converted-space">    </span>| +<span class="Apple-converted-space">      </span>| + <span class="Apple-converted-space">      </span>| 1<span class="Apple-converted-space">    </span>| nT</span><span class="s1">&lt;</span><span class="s6">sup</span><span class="s1">&gt;</span><span class="s2">a</span><span class="s1">&lt;/</span><span class="s6">sup</span><span class="s1">&gt;</span><span class="s2">, a </span><span class="s6">&amp;le;</span><span class="s2"> 2 | TL +<span class="Apple-converted-space">  </span>replay<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">| semipar | +<span class="Apple-converted-space">      </span>| 0 <span class="Apple-converted-space">      </span>| T<span class="Apple-converted-space">    </span>| n T <span class="Apple-converted-space">            </span>| ProgNN, DEN <span class="Apple-converted-space">               </span></span></p>
<p class="p3"><span class="s2">| semipar | +<span class="Apple-converted-space">      </span>| + <span class="Apple-converted-space">      </span>| T<span class="Apple-converted-space">    </span>| n T</span><span class="s1">&lt;</span><span class="s6">sup</span><span class="s1">&gt;</span><span class="s2">2</span><span class="s1">&lt;/</span><span class="s6">sup</span><span class="s1">&gt;</span><span class="s2"> | Sequential Multitask <span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">| semipar | +<span class="Apple-converted-space">      </span>| + <span class="Apple-converted-space">      </span>| T<span class="Apple-converted-space">    </span>| nT<span class="Apple-converted-space">              </span>| ProgL Networks <span class="Apple-converted-space">       </span></span></p>
<p class="p3"><span class="s2">| nonpar<span class="Apple-converted-space">  </span>| +<span class="Apple-converted-space">      </span>| + <span class="Apple-converted-space">      </span>| n<span class="Apple-converted-space">    </span>| nT<span class="Apple-converted-space">              </span>| ProgL Forests<span class="Apple-converted-space">         </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- parametric, replay, space, time, forward, backwards, examples --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- apples to apples comparisons are only possible within a row of the table --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p3"><span class="s2">### Some nuances<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Sometimes, $f$ might not know that the task has changed<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- This framework cannot easily deal with distribution drift, or reinforcement learning</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p3"><span class="s2">name:rep</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Outline<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Learning</span></p>
<p class="p3"><span class="s2">- [Ensembling](#rep)</span></p>
<p class="p3"><span class="s2">- [Experiments](#exp)</span></p>
<p class="p3"><span class="s2">- [Theory](#theory)</span></p>
<p class="p3"><span class="s2">- [Brains](#neuro)</span></p>
<p class="p3"><span class="s2">- [Discussion](#disc)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">###<span class="Apple-converted-space">  </span>Learning Taxonomy<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 100%](images/learning-taxonomy.svg)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Ways Tasks can Differ</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">| Component | Notation | Examples |</span></p>
<p class="p3"><span class="s2">| :--- | :--- | :---<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">| Query Space | $\mathcal{Q}$ | new keyboard introduced</span></p>
<p class="p3"><span class="s2">| Action Space | $\mathcal{A}$ | class incremental, task incremental</span></p>
<p class="p3"><span class="s2">| Measurement Space | $\mathcal{Z}$ | another modality</span></p>
<p class="p3"><span class="s2">| Statistical Model | $\mathcal{P}$ | Gaussian to Log-Gaussian</span></p>
<p class="p3"><span class="s2">| Hypotheses | $\mathcal{H}$ | linear functions</span></p>
<p class="p3"><span class="s2">| Risk | $R$ | expected loss</span></p>
<p class="p3"><span class="s2">| Algorithm Space | $\mathcal{F}$ | SVM</span></p>
<p class="p3"><span class="s2">| Distribution | $P$ | mean shift</span></p>
<p class="p3"><span class="s2">| Task Awareness | $T_i$ | {aware, oblivious, ambivalent}</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$2^8 \times 3 \approx 800$ ways tasks can differ. <span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p3"><span class="s2">name:rep</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Outline<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- [Learning](#learn)</span></p>
<p class="p3"><span class="s2">- Ensembling</span></p>
<p class="p3"><span class="s2">- [Experiments](#exp)</span></p>
<p class="p3"><span class="s2">- [Theory](#theory)</span></p>
<p class="p3"><span class="s2">- [Brains](#neuro)</span></p>
<p class="p3"><span class="s2">- [Discussion](#disc)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Composable Hypotheses<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">.center[ .ye[$h(\cdot) := w \circ v \circ u (\cdot) = w(v(u(\cdot)))$]]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Let $u$ be .ye[transformer] data to a new representation,<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$$ u : \mathcal{Q}<span class="Apple-converted-space">  </span>\to \tilde{\mathcal{Q}}$$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Let $v$ be .ye[voter] which operate on the transformed data outputs votes (score functions, posteriors) on all possible actions<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$$ v : \tilde{\mathcal{Q}} \to \mathcal{V}$$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Let $w$ be .ye[decider] which decides which actions to take on the basis of the votes<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$$ w : \mathcal{V} \to \mathcal{A}$$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 100%](images/single_decomposable_hypothesis.png)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- TODO@ali: can we use an svg here? or a higher res png if you can't get a vector graphic? --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Simple Examples</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Linear Discriminant Analysis (shallow)</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- $u$: projection onto a line<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- $v$: fraction of points per over/under threshold</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- $w$: maximum a posteriori class<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">--</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Decision Tree (deep)</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space"> </span>- $u$: union of polytopes</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space"> </span>- $v$: fraction of points per class per leaf node</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space"> </span>- $w$: maximum a posteriori class<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Predictive Ensembling</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Ensemble votes from multiple voters in a decider</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>$$</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>w \circ</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>\begin{bmatrix}</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>v_1 \circ u_1 \\\\</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>v_2 \circ u_2 \\\\</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>\vdots \\\\</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>v_m \circ u_m<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>\end{bmatrix}</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>$$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 100%](images/predictive_ensembling.png)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">#### Predictive Ensembling Example</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Decision Forest<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- $u_b$ for $B$ trees: union of overlapping polytopes</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- $v_b$ for $B$ trees: fraction of points per class per leaf node</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- $w$: maximum a posteriori class averaging over trees<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Key Idea<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- .ye[Different transformers can composed with<span class="Apple-converted-space">  </span>voters]</span></p>
<p class="p3"><span class="s2">- Learn many different transformers $u_t(\cdot)$'s<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- For each $u\_t$, learn voter per task $v\_{t,t'}$'s<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- Use the decider to weight the various options<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- This is .ye[ensembling representations].</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Notes</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- We learn new representation for each task.<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- Dimensionality of internal representation grows linearly with number of tasks.</span></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space">  </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Representational Ensembling</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Ensemble representations from multiple transformers in a voter</span></p>
<p class="p3"><span class="s2">- Assume $m$ transformers and $n$ voters</span></p>
<p class="p3"><span class="s2">- Let $u =<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>\begin{bmatrix}</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>u_1 \\\\</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>u_2 \\\\</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>\vdots \\\\</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>u_m<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>\end{bmatrix}$, and<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space"> </span>$</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>w \circ</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>\begin{bmatrix}</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>v_1 \circ u \\\\</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>v_2 \circ u \\\\</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>\vdots \\\\</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>v_n \circ u<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>\end{bmatrix}</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 100%](images/representational_ensembling.png)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">#### Representational Ensembling Examples<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Uncertainty Forests<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- $u$: tree structures</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- $v$: posterior estimators</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- $w$: max<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- Deep Nets<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- $u$: "backbone" (all but last layer)</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- $v$: softmax layer</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- $w$: max<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Composable Learning</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p1"><span class="s1">&lt;</span><span class="s2">br</span><span class="s1">&gt;</span><span class="s3"><span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">|<span class="Apple-converted-space">  </span>Scenario | Composition<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">|<span class="Apple-converted-space">  </span>:--- | :---<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">| Single task learning | $ h(\cdot) = w \circ v \circ u (\cdot)$</span></p>
<p class="p3"><span class="s2">| Multiple independent task learning | $ h_t(\cdot) = w_t \circ<span class="Apple-converted-space">  </span>v_t \circ u_t (\cdot)$<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">| Single task ensemble learning |$ h(\cdot) = w \circ \bigcup_t [ v_t \circ u_t (\cdot)] $<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">| Multitask learning | $ h_t(\cdot) = w_t \circ<span class="Apple-converted-space">  </span>v<span class="Apple-converted-space">  </span>\circ \bigcup_t<span class="Apple-converted-space">  </span>u_t (\cdot)$</span></p>
<p class="p3"><span class="s2">| .ye[Multitask ensemble representation learning]<span class="Apple-converted-space">  </span>| $ h\_t(\cdot) = w\_t \circ<span class="Apple-converted-space">  </span>\bigcup\_{t'}<span class="Apple-converted-space">  </span>[v\_{t,t'}<span class="Apple-converted-space">  </span>\circ<span class="Apple-converted-space">    </span>u\_{t'} (\cdot) ] $</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space">  </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Lifelong Learning Schema</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 100%](images/learning-schemas.png)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Any learner with an explicit internal representation is ok,<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- e.g.,<span class="Apple-converted-space">  </span>decision trees, decision forests, deep networks<span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2">&lt;!-- - SVM's are not obviously --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space">  </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### General Representations<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Transformers learn representations<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- We desire representations that are sufficient for one task, and<span class="Apple-converted-space">  </span>useful for other tasks<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- Decision trees, decision forests, and deep nets (with ReLu nodes) .ye[partition] feature space into polytopes</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 100%](images/deep-polytopes.png)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;img src="images/deep-polytopes.png" style="width:500px;"/&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Partition and Vote<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- TODO@ali make this slide--&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Given query space $\mathcal{Q}$, say $\mathbb{R}^d$ and $n$ samples, for example $(x\_i, y\_i)_{i = 1}^{n}$, where $x_i \in \mathbb{R}^d$ and $y_i$ could be labels</span></p>
<p class="p3"><span class="s2">- Transfomer $u$ partitions $\mathcal{Q}$, mapping $x_i$ to its corresponding cell in the partition</span></p>
<p class="p3"><span class="s2">- Voter $v$ scores actions in each cell (e.g. empirical posterior distribution) given how they are populated by transformer<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- For a given test query $x$,<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>- $u$ maps $x$ to its cell</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>- $v$ votes on actions in this cell</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>- decider $w$ chooses action based on $v$'s votes (e.g. arg max)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Ensemble, Partition and Vote<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Get a transformer class $U$, with each $u \in U$ partitioning $\mathbb{R}^d$ differently</span></p>
<p class="p3"><span class="s2">- Get a voter class $V$, with each $v \in V$ voting differently on each cell in a given partition, and make it vote on every partition<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- Decider $w$ ensembles the votes on actions from each voter to decide on an action given a test query $x$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Lifelong Learning Algorithm</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">For each new task,<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">1. learn a new representation function,<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">2. apply it to all data from all tasks: the updated representation for everything is the composition of this new representation with existing representations. <span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">4. update all decision rules using this representation.</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">Notes:</span></p>
<p class="p3"><span class="s2">- This linearly increases representation capacity.</span></p>
<p class="p3"><span class="s2">- Without increasing representation capacity, performance on all tasks will necessarily drop to chance levels eventually as number of tasks increases.</span></p>
<p class="p3"><span class="s2">- Thus, fixed capacity systems can only lifelong learn insofar as they are inefficient (unnecessarily big) for individual tasks.</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- TODO@jv: somewhere must introduce the concept of adjusting representations --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Pseudocode<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Given<span class="Apple-converted-space">  </span>$\color{magenta}{j-1}$ transformers learned from the previous $\color{magenta}{j-1}$ datasets and<span class="Apple-converted-space">  </span>a new $\color{yellow}{j^{th}}$ dataset with task label $\color{yellow}{t_j}$, do:</span></p>
<p class="p3"><span class="s2">- learn a new transformer using $\color{yellow}{j^{th}}$ data</span></p>
<p class="p3"><span class="s2">- .magenta[reverse transfer update] for each of the $\color{magenta}{j-1}$ previous tasks:<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>1. transform a subset of the data through the $\color{yellow}{j^{th}}$ transformer</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">     </span>(this requires having stored some of the data)</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>3. learn a new voter using the $\color{yellow}{j^{th}}$ representation of data</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>4. update decision rules by appending this additional voter</span></p>
<p class="p3"><span class="s2">- .ye[forward transfer update] for all data associated with $\color{yellow}{j^{th}}$ task:</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>1. transform a subset of the data through the $\color{yellow}{j^{th}}$ transformer</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>2. transform through each of the $\color{magenta}{j-1}$ existing transformers<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>3. learn a new voter for all $j$ transformers<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>4. make decision rule by averaging over $j$ voters</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p3"><span class="s2">name:results<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Outline<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- [Learning](#learn)</span></p>
<p class="p3"><span class="s2">- [Ensembling](#rep)</span></p>
<p class="p3"><span class="s2">- Experiments</span></p>
<p class="p3"><span class="s2">- [Theory](#theory)</span></p>
<p class="p3"><span class="s2">- [Brains](#neuro)</span></p>
<p class="p3"><span class="s2">- [Discussion](#disc)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### A Transfer Example</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- .ye[XOR]</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- Samples in the (0,0) and (1,1) quadrants are purple <span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- samples in the (0,1) and (1,0) quadrants are green<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- .lb[N-XOR]</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- Samples in the (0,0) and (1,1) quadrants are green <span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- samples in the (0,1) and (1,0) quadrants are purple<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- Optimal decision boundaries for both problems are coordinate axes</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"images/gaussian-xor-nxor.svg"</span><span class="s3"> </span><span class="s4">style</span><span class="s3">=</span><span class="s2">"width:475px"</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"center"</span><span class="s1">/&gt;</span><span class="s3"><span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### XOR vs NXOR Transfer Efficiency</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 100%](images/xor-te.svg)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Lots of Transfer Efficiency</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 100%](images/lotsa-te.svg)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!--<span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2">### Different # of Classes<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;img src="images/spiral-all.png"<span class="Apple-converted-space">  </span>style="height:500px;"&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- ## Consider an<span class="Apple-converted-space">  </span>example --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### CIFAR 10x10</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">.pull-left[</span></p>
<p class="p3"><span class="s2">- *CIFAR 100* is a popular image classification dataset with 100 classes of images.<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- 500 training images and 100 testing images per class.</span></p>
<p class="p3"><span class="s2">- All images are 32x32 color images.</span></p>
<p class="p3"><span class="s2">- CIFAR 10x10 breaks the 100-class task problem into 10 tasks, each with 10-class.</span></p>
<p class="p3"><span class="s2">]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">.pull-right[</span></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"images/l2m_18mo/cifar-10.png"</span><span class="s3"> </span><span class="s4">style</span><span class="s3">=</span><span class="s2">"position:absolute; left:450px; width:400px;"</span><span class="s1">/&gt;</span></p>
<p class="p3"><span class="s2">]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!--<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">### Forward Transfer Efficiency</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">- y-axis indicates .ye[forward transfer efficiency] (FTE),<span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>- which is the ratio of "single task error" to "error using past tasks"</span></p>
<p class="p5"><span class="s2">- each algorithm has a line</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>- if the line .ye[increases], that means it is doing "forward transfer"</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space"> </span>--&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">Lifelong Forests and Networks consistently demonstrate .ye[forward transfer] for every task.</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 100%](images/cifar-100-FTE.svg)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- left: resource building<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- right: resource recruiting</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!--<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">### Backward Transfer Efficiency</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">- y-axis indicates .ye[backward transfer efficiency] (BTE),<span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>- which is the ratio of "single task error" to "error using future tasks"</span></p>
<p class="p5"><span class="s2">- each task will have a line</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>- if the line .ye[increases], that means it is doing "backward transfer"<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">--&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">Lifelong Forests and Networks .ye[uniquely exhibits backward transfer].</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 100%](images/cifar-100-BTE.svg)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- left: resource building<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- right: resource recruiting</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### L2F &amp; L2N<span class="Apple-converted-space">  </span>transfer on .ye[every task]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 60%](images/TE.svg)<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Language Identification</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- 8,194,317 sentences from wikipedia (downloaded from facebook).<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- 156 languages</span></p>
<p class="p3"><span class="s2">- Trained using unsupervised FastText embedding</span></p>
<p class="p3"><span class="s2">- words, 2-4 char n-grams embedded into 16 dimensions</span></p>
<p class="p3"><span class="s2">- selected 30 languages</span></p>
<p class="p3"><span class="s2">- break into batches of 3 "related" languages</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 100%](images/30-languages.png)<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">###<span class="Apple-converted-space">  </span>Backward Transfer</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 60%](images/language.svg)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- Note RTE &amp;gt;5 for task 4.</span></p>
<p class="p5"><span class="s2">--&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Web-Search Categorization</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">.pull-left[</span></p>
<p class="p3"><span class="s2">- Same data as above</span></p>
<p class="p3"><span class="s2">- labels now correspond to Microsoft Bing "dominant type"</span></p>
<p class="p3"><span class="s2">- 10k training<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- 1k testing entities</span></p>
<p class="p3"><span class="s2">- 20 classes<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- each with </span><span class="s6">&amp;ge;</span><span class="s2">11k samples</span></p>
<p class="p3"><span class="s2">- 4 classes per task</span></p>
<p class="p3"><span class="s2">]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">.pull-right[</span></p>
<p class="p3"><span class="s2">![:scale 100%](images/bing-dominant-types.png)<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Backward Transfer</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 60%](images/web.svg)<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Kernel Density Graphs - Partition/Vote</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 60%](images/KDG_3x3.png)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Kernel Density Graphs - Partition/Vote</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 60%](images/rf_v_kdf.png)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Kernel Density Graphs - Partition/Vote</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 60%](images/dn_v_kdn.png)</span></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">## Outline</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- [Learning](#learn)</span></p>
<p class="p3"><span class="s2">- [Ensembling](#rep)</span></p>
<p class="p3"><span class="s2">- [Experiments](#exp)</span></p>
<p class="p3"><span class="s2">- Theory</span></p>
<p class="p3"><span class="s2">- [Brains](#neuro)</span></p>
<p class="p3"><span class="s2">- [Discussion](#disc)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### What do classifiers do?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p1"><span class="s1">&lt;</span><span class="s2">br</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">learn: given $(x_i,y_i)$, for $i \in [n]$, where $y \in \lbrace 0,1 \rbrace$</span></p>
<p class="p3"><span class="s2">1. partition feature space into "parts",</span></p>
<p class="p3"><span class="s2">2. compute plurality<span class="Apple-converted-space">  </span>of points in each part.</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">predict: given $x$</span></p>
<p class="p3"><span class="s2">2. find its part,<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">3. report the plurality vote in its part.</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### What can regressors do?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p1"><span class="s1">&lt;</span><span class="s2">br</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">learn: given $(x_i,y_i)$, for $i \in [n]$, where $y \in \mathbb{R}$</span></p>
<p class="p3"><span class="s2">1. partition feature space into "parts",</span></p>
<p class="p3"><span class="s2">2. compute average of points in each part.</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">predict: given $x$</span></p>
<p class="p3"><span class="s2">2. find its part,</span></p>
<p class="p3"><span class="s2">3. report the average vote in its part.</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### The fundamental theorem of statistical pattern recognition</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">If each part is:</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">1. small enough, and<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">2. has enough points in it,<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">then given enough data, one can learn *perfectly, no matter what*!<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$$\mathcal{E}\(f_n) \rightarrow \mathcal{E}^*,$$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">where $\mathcal{E}^*$is Bayes optimal.</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">-- Stone, 1977</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- NB: the parts can be overlapping (as in kNN) or not (as in histograms) --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### The fundamental .ye[theorem] of transfer learning</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">If each cell is:</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- small enough, and<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- has enough points in it,<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">then given enough data, one can .ye[transfer learn] *no matter what*!<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">-- jovo, 2020</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">Specifically, this means:</span></p>
<p class="p3"><span class="s2">- as $n_0, n_1 \to \infty$, TE is at least $1$, $\mathcal{E}(f_n) \to \mathcal{E}^*$<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p3"><span class="s2">name:neuro<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Outline<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- [Learning](#learn)</span></p>
<p class="p3"><span class="s2">- [Ensembling](#rep)</span></p>
<p class="p3"><span class="s2">- [Experiments](#exp)</span></p>
<p class="p3"><span class="s2">- [Theory](#theory)</span></p>
<p class="p3"><span class="s2">- Brains</span></p>
<p class="p3"><span class="s2">- [Discussion](#disc)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">###<span class="Apple-converted-space">  </span>Neurobiological Insights</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">1. Lifelong learning happens in two phases:</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>1. a .ye[juvenile] phase, in which capacity is building</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>2. an .ye[adult] phase, in which capacity is basically fixed</span></p>
<p class="p3"><span class="s2">1. Implications</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- adult learning essentially recombines knowledge from juvenile, but cannot add knowledge willy-nilly<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- the role of adult brain is to recruit resources to maximize transfer (and minimize forgetting important things)</span></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space">  </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Neurobiology Background</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- All brains start with 1 neurons, and *increase neural capacity* during embroynic and juvenile developmental stages<span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2">&lt;!-- - In many taxa, # of neurons increases throughout development<span class="Apple-converted-space">  </span>--&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- - In all taxa, # synapses increase through juvenile state --&gt;</span></p>
<p class="p3"><span class="s2">- During development, basic concepts are established</span></p>
<p class="p5"><span class="s2">&lt;!-- - If natural stimuli are unavailable developmentally, such concepts never form<span class="Apple-converted-space">  </span>--&gt;</span></p>
<p class="p3"><span class="s2">- In adulthood, animals learn new concepts<span class="Apple-converted-space">  </span>by recombination<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- Concepts that are not combinations can never form<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- So fixed capacity system only happens after significant training<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">iframe</span><span class="s3"> </span><span class="s4">width</span><span class="s3">=</span><span class="s2">"560"</span><span class="s3"> </span><span class="s4">height</span><span class="s3">=</span><span class="s2">"315"</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"https://www.youtube.com/embed/C2q3Dqv9PEA?start=5"</span><span class="s3"> </span><span class="s4">frameborder</span><span class="s3">=</span><span class="s2">"0"</span><span class="s3"> </span><span class="s4">allow</span><span class="s3">=</span><span class="s2">"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"</span><span class="s3"> </span><span class="s4">allowfullscreen</span><span class="s1">&gt;&lt;/</span><span class="s6">iframe</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### How do brains learn?<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- "Partitioning", as implemented by a network, corresponds to only a subset of nodes responding to any given input<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"images/rock20/Side-black.gif"</span><span class="s3"> </span><span class="s4">style</span><span class="s3">=</span><span class="s2">"height:230px;"</span><span class="s1">/&gt;</span></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"images/rock20/Front_of_Sensory_Homunculus.gif"</span><span class="s3"> </span><span class="s4">style</span><span class="s3">=</span><span class="s2">"height:230px;"</span><span class="s1">/&gt;</span></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"images/rock20/Rear_of_Sensory_Homunculus.jpg"</span><span class="s3"> </span><span class="s4">style</span><span class="s3">=</span><span class="s2">"height:230px;"</span><span class="s1">/&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- - Each connectome dynamically reconfigures at multiple time-scales to store novel information<span class="Apple-converted-space">  </span>--&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- - Memory consolidation requires a physical reconfiguration implemented by a sequence of immediate early genes (IEGs) --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### How do brains learn?<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- "Partitioning", as implemented by a network, corresponds to only a subset of nodes responding to any given input<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- A brain's connectome implements a partitioning of feature space<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;iframe width="560" height="315" src="videos/zebrafish_em_traces.m4v" frameborder="0" allow="encrypted-media" allowfullscreen&gt;&lt;/iframe&gt; --&gt;</span></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">iframe</span><span class="s3"> </span><span class="s4">width</span><span class="s3">=</span><span class="s2">"560"</span><span class="s3"> </span><span class="s4">height</span><span class="s3">=</span><span class="s2">"315"</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"https://www.youtube.com/embed/ykIj-9a_ss4?start=495"</span><span class="s3"> </span><span class="s4">frameborder</span><span class="s3">=</span><span class="s2">"0"</span><span class="s3"> </span><span class="s4">allow</span><span class="s3">=</span><span class="s2">"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"</span><span class="s3"> </span><span class="s4">allowfullscreen</span><span class="s1">&gt;&lt;/</span><span class="s6">iframe</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### How do brains learn?<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- "Partitioning", as implemented by a network, corresponds to only a subset of nodes responding to any given input<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- A brain's connectome implements a partitioning of feature space<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- Each connectome dynamically reconfigures at multiple time-scales to store novel information<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;iframe width="560" height="315" src="videos/zebrafish_ca.m4v" frameborder="0" allow="encrypted-media" allowfullscreen&gt;&lt;/iframe&gt; --&gt;</span></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">iframe</span><span class="s3"> </span><span class="s4">width</span><span class="s3">=</span><span class="s2">"560"</span><span class="s3"> </span><span class="s4">height</span><span class="s3">=</span><span class="s2">"315"</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"https://www.youtube.com/embed/lppAwkek6DI"</span><span class="s3"> </span><span class="s4">frameborder</span><span class="s3">=</span><span class="s2">"0"</span><span class="s3"> </span><span class="s4">allow</span><span class="s3">=</span><span class="s2">"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"</span><span class="s3"> </span><span class="s4">allowfullscreen</span><span class="s1">&gt;&lt;/</span><span class="s6">iframe</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### How do brains learn?<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- "Partitioning", as implemented by a network, corresponds to only a subset of nodes responding to any given input<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- A brain's connectome implements a partitioning of feature space<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- Each connectome dynamically reconfigures at multiple time-scales to store novel information<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- Memory consolidation requires a physical reconfiguration implemented by a sequence of immediate early genes (IEGs)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;video width="560" height="420" controls&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;source src="videos/zebrafish_ca.m4v" type="video/mp4"&gt;</span></p>
<p class="p5"><span class="s2">&lt;/video&gt;<span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2">--&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### NeuroExperiments<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- How does the brain select which neurons/synapses to modify to store new information?</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- The choice should maximize transfer efficiency<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- We can simultaneously observe neural and IEG activity during and after a learning event (e.g., a foot shock)</span></p>
<p class="p3"><span class="s2">- We can identify the neural ensembles primed to learn with Arc-GFP<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- We can identify sets of ensembles of neural activity using jRGECO1a</span></p>
<p class="p3"><span class="s2">- We can then discover the relationship between these two sets of ensembles of neurons</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p3"><span class="s2">name:disc<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Outline<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- [Learning](#learn)</span></p>
<p class="p3"><span class="s2">- [Ensembling](#rep)</span></p>
<p class="p3"><span class="s2">- [Experiments](#exp)</span></p>
<p class="p3"><span class="s2">- [Theory](#theory)</span></p>
<p class="p3"><span class="s2">- [Brains](#neuro)</span></p>
<p class="p3"><span class="s2">- Discussion</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Key Claims</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">1. If you don't<span class="Apple-converted-space">  </span>transfer, you haven't lifelong learned, rather, you've .ye[sequentially compressed].</span></p>
<p class="p3"><span class="s2">2. We propose the only algorithm in the literature the .ye[demonstrates]<span class="Apple-converted-space">  </span>lifelong learning, ie, sequential transfer.</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Summary of contributions</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">1. Formalized Lifelong Learning as generalization of classical machine learning</span></p>
<p class="p3"><span class="s2">1. Introduced forward and backward transfer efficiency</span></p>
<p class="p3"><span class="s2">1. Proposed<span class="Apple-converted-space">  </span>omnidirectional transfer learning<span class="Apple-converted-space">  </span>framework by ensembling<span class="Apple-converted-space">  </span>representations</span></p>
<p class="p3"><span class="s2">1. Implemented Lifelong Learning Forests and Networks<span class="Apple-converted-space">  </span>(L2F/L2N)<span class="Apple-converted-space">  </span>([code](https://github.com/neurodata/progressive-learning))</span></p>
<p class="p3"><span class="s2">1. Demonstrated L2F/L2N<span class="Apple-converted-space">  </span>exhibits<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>1. positive forward transfer<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>1. positive backward transfer (uniquely)</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>1. positive strong transfer (uniquely)</span></p>
<p class="p3"><span class="s2">1. Proved consistency and robustness in transfer and lifelong learning</span></p>
<p class="p3"><span class="s2">1. Described equivalence between Decision Forests and Deep Nets</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Extension #1: Streaming<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Current implementation requires all data per task are batched</span></p>
<p class="p3"><span class="s2">- Could stream trees per sample</span></p>
<p class="p3"><span class="s2">- Would provide truly continual transfer</span></p>
<p class="p3"><span class="s2">- Collaborators: JHU seedling (Braverman)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Extension #2: Compression<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Current implementation linearly grows internal representation with each new task</span></p>
<p class="p3"><span class="s2">- Could compress internal representation after training to achieve a fixed representation space (e.g., using coresets)</span></p>
<p class="p3"><span class="s2">- For forests, this could happen at the node or tree level<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- Collaborators: JHU seedling (Braverman)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Extension #3: Replay</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Current implementation requires storing some data to achieve *backward* transfer</span></p>
<p class="p3"><span class="s2">- Could leverage replay to reduce dependency of increasing data storage</span></p>
<p class="p3"><span class="s2">- Collaborators: Baylor (Tolias) &amp; McNaughton (UCI+UCSD)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Extension #4: Agent</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Current implementation's action are labels and do not impact future data</span></p>
<p class="p3"><span class="s2">- Could integrate into larger L2 system that incorporates agent based learning</span></p>
<p class="p3"><span class="s2">- Collaborators: Aguilar-Simon (Teledyne)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Other possible extensions<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">2. Allow non-discrete tasks<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">4. Support task-oblivious setting<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">5. Support multi-modal and cross-modal</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- 1. Allow fully sequential data<span class="Apple-converted-space">  </span>--&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- 2. Allow fixed capacity representation --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- 3. Allow replay to support fixed capacity<span class="Apple-converted-space">  </span>--&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- 4. Allow agent based extension --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- 1. No implementation using deep nets <span class="Apple-converted-space">      </span>--&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- Tasks must be known (no implementation that imputes task ID) --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- Feature space must be the same for all tasks (no data fusion step) --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- 6. Only unimodal data supported (no multimodal implementation) --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- 1. Must grow rather than recruit new internal representations (no pre-training implemented) <span class="Apple-converted-space">      </span>--&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- 1. Requires storing some samples to achieve backwards transfer (no replay capacity) --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- 1. No support for specific modalities (e.g., images) --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">.small[</span></p>
<p class="p3"><span class="s2">### Publications</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">1. R. Mehta et al. A General Theory of the Task Learnable, 2020.<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">1. J. T. Vogelstein et al. [A general approach to progressive learning](https://arxiv.org/abs/2004.12908), arXiv, 2020</span></p>
<p class="p3"><span class="s2">1. C. E. Priebe et al. [Modern Machine Learning: Partition and Vote](https://doi.org/10.1101/2020.04.29.068460), 2020.<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">1. R Guo, et al. [Estimating Information-Theoretic Quantities with Uncertainty Forests](https://arxiv.org/abs/1907.00325). arXiv, 2019.</span></p>
<p class="p3"><span class="s2">1. R. Perry, et al. [Manifold Forests: Closing the Gap on Neural Networks](https://openreview.net/forum?id=B1xewR4KvH). arXiv, 2019.</span></p>
<p class="p3"><span class="s2">1. C. Shen and J. T. Vogelstein. [Decision Forests Induce Characteristic Kernels](https://arxiv.org/abs/1812.00029). arXiv, 2019</span></p>
<p class="p3"><span class="s2">1. M. Madhya, et al. [Geodesic Learning via Unsupervised Decision Forests](https://arxiv.org/abs/1907.02844). arXiv, 2019.</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Conferences<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">1. J.T. Vogelstein et al. A biological implementation of lifelong learning in the pursuit of artificial general intelligence.<span class="Apple-converted-space">  </span>NAISys, 2020.</span></p>
<p class="p3"><span class="s2">2. B. Pedigo et al.<span class="Apple-converted-space">  </span>A quantitative comparison of a complete connectome to artificial intelligence architectures. NAISys, 2020.</span></p>
<p class="p3"><span class="s2">]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Acknowledgements</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/ebridge.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Eric Bridgeford&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/pedigo.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Ben Pedigo&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/jaewon.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Jaewon Chung&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"small-container"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"faces/yummy.jpg"</span><span class="s1">/&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"centered"</span><span class="s1">&gt;</span><span class="s3">yummy</span><span class="s1">&lt;/</span><span class="s6">div</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">div</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"small-container"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"faces/lion.jpg"</span><span class="s1">/&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"centered"</span><span class="s1">&gt;</span><span class="s3">lion</span><span class="s1">&lt;/</span><span class="s6">div</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">div</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"small-container"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"faces/violet.jpg"</span><span class="s1">/&gt;</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">div</span><span class="s2"> </span><span class="s4">class</span><span class="s2">=</span><span class="s7">"centered"</span><span class="s1">&gt;</span><span class="s2">baby girl</span><span class="s1">&lt;/</span><span class="s6">div</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">div</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"small-container"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"faces/family.jpg"</span><span class="s1">/&gt;</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">div</span><span class="s2"> </span><span class="s4">class</span><span class="s2">=</span><span class="s7">"centered"</span><span class="s1">&gt;</span><span class="s2">family</span><span class="s1">&lt;/</span><span class="s6">div</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">div</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"small-container"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"faces/earth.jpg"</span><span class="s1">/&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"centered"</span><span class="s1">&gt;</span><span class="s3">earth</span><span class="s1">&lt;/</span><span class="s6">div</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">div</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"small-container"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"faces/milkyway.jpg"</span><span class="s1">/&gt;</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">div</span><span class="s2"> </span><span class="s4">class</span><span class="s2">=</span><span class="s7">"centered"</span><span class="s1">&gt;</span><span class="s2">milkyway</span><span class="s1">&lt;/</span><span class="s6">div</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">div</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">##### JHU</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"small-container"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"faces/cep.png"</span><span class="s1">/&gt;</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">div</span><span class="s2"> </span><span class="s4">class</span><span class="s2">=</span><span class="s7">"centered"</span><span class="s1">&gt;</span><span class="s2">Carey Priebe</span><span class="s1">&lt;/</span><span class="s6">div</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">div</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/randal.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Randal Burns&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/cshen.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Cencheng Shen&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/bruce_rosen.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Bruce Rosen&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/kent.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Kent Kiehl&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/mim.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Michael Miller&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/dtward.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Daniel Tward&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/vikram.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Vikram Chandrashekhar&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/drishti.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Drishti Mannan&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"small-container"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"faces/jesse.jpg"</span><span class="s1">/&gt;</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">div</span><span class="s2"> </span><span class="s4">class</span><span class="s2">=</span><span class="s7">"centered"</span><span class="s1">&gt;</span><span class="s2">Jesse Patsolic</span><span class="s1">&lt;/</span><span class="s6">div</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">div</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/falk_ben.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Benjamin Falk&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/kwame.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Kwame Kutten&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/perlman.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Eric Perlman&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/loftus.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Alex Loftus&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/bcaffo.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Brian Caffo&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/minh.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Minh Tang&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/avanti.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Avanti Athreya&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/vince.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Vince Lyzinski&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/dpmcsuss.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Daniel Sussman&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/youngser.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Youngser Park&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/shangsi.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Shangsi Wang&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/tyler.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Tyler Tomita&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/james.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;James Brown&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/disa.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Disa Mhembere&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/gkiar.jpg"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Greg Kiar&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/jeremias.png"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Jeremias Sulam&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"small-container"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"faces/meghana.png"</span><span class="s1">/&gt;</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">div</span><span class="s2"> </span><span class="s4">class</span><span class="s2">=</span><span class="s7">"centered"</span><span class="s1">&gt;</span><span class="s2">Meghana Madhya</span><span class="s1">&lt;/</span><span class="s6">div</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">div</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space">  </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- &lt;div class="small-container"&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;img src="faces/percy.png"/&gt;</span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>&lt;div class="centered"&gt;Percy Li&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">&lt;/div&gt;</span></p>
<p class="p5"><span class="s2">--&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"small-container"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"faces/hayden.png"</span><span class="s1">/&gt;</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">div</span><span class="s2"> </span><span class="s4">class</span><span class="s2">=</span><span class="s7">"centered"</span><span class="s1">&gt;</span><span class="s2">Hayden Helm</span><span class="s1">&lt;/</span><span class="s6">div</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">div</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"small-container"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"faces/rguo.jpg"</span><span class="s1">/&gt;</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">div</span><span class="s2"> </span><span class="s4">class</span><span class="s2">=</span><span class="s7">"centered"</span><span class="s1">&gt;</span><span class="s2">Richard Gou</span><span class="s1">&lt;/</span><span class="s6">div</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">div</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"small-container"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"faces/ronak.jpg"</span><span class="s1">/&gt;</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">div</span><span class="s2"> </span><span class="s4">class</span><span class="s2">=</span><span class="s7">"centered"</span><span class="s1">&gt;</span><span class="s2">Ronak Mehta</span><span class="s1">&lt;/</span><span class="s6">div</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">div</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"small-container"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"faces/jayanta.jpg"</span><span class="s1">/&gt;</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">div</span><span class="s2"> </span><span class="s4">class</span><span class="s2">=</span><span class="s7">"centered"</span><span class="s1">&gt;</span><span class="s2">Jayanta Dey</span><span class="s1">&lt;/</span><span class="s6">div</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">div</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"small-container"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"faces/will.jpg"</span><span class="s1">/&gt;</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">div</span><span class="s2"> </span><span class="s4">class</span><span class="s2">=</span><span class="s7">"centered"</span><span class="s1">&gt;</span><span class="s2">Will LeVine</span><span class="s1">&lt;/</span><span class="s6">div</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">div</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">##### Microsoft Research</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"small-container"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"faces/chwh-180x180.jpg"</span><span class="s1">/&gt;</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">div</span><span class="s2"> </span><span class="s4">class</span><span class="s2">=</span><span class="s7">"centered"</span><span class="s1">&gt;</span><span class="s2">Chris White</span><span class="s1">&lt;/</span><span class="s6">div</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">div</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"small-container"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"faces/weiwei.jpg"</span><span class="s1">/&gt;</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">div</span><span class="s2"> </span><span class="s4">class</span><span class="s2">=</span><span class="s7">"centered"</span><span class="s1">&gt;</span><span class="s2">Weiwei Yang</span><span class="s1">&lt;/</span><span class="s6">div</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">div</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"small-container"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"faces/jolarso150px.png"</span><span class="s1">/&gt;</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">div</span><span class="s2"> </span><span class="s4">class</span><span class="s2">=</span><span class="s7">"centered"</span><span class="s1">&gt;</span><span class="s2">Jonathan Larson</span><span class="s1">&lt;/</span><span class="s6">div</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">div</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">div</span><span class="s3"> </span><span class="s4">class</span><span class="s3">=</span><span class="s2">"small-container"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"faces/brtower-180x180.jpg"</span><span class="s1">/&gt;</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">div</span><span class="s2"> </span><span class="s4">class</span><span class="s2">=</span><span class="s7">"centered"</span><span class="s1">&gt;</span><span class="s2">Bryan Tower</span><span class="s1">&lt;/</span><span class="s6">div</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">div</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">##### DARPA L2M</span></p>
<p class="p5"><span class="s2">&lt;!-- Hava, Ben, Robert, Jennifer, Ted. --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">{[BME](https://www.bme.jhu.edu/),[CIS](http://cis.jhu.edu/), [ICM](https://icm.jhu.edu/), [KNDI](http://kavlijhu.org/)}@[JHU](https://www.jhu.edu/) | [neurodata](https://neurodata.io)</span></p>
<p class="p1"><span class="s1">&lt;</span><span class="s2">br</span><span class="s1">&gt;</span></p>
<p class="p3"><span class="s2">[jovo</span><span class="s6">&amp;#0064;</span><span class="s2">jhu.edu](mailto:j1c@jhu.edu) | </span><span class="s1">&lt;</span><span class="s6">http://neurodata.io/talks</span><span class="s1">&gt;</span><span class="s2"> | [@neuro_data](https://twitter.com/neuro_data)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">div</span><span class="s1">&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- &lt;img src="images/funding/nsf_fpo.png" STYLE="HEIGHT:95px;"/&gt; --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- &lt;img src="images/funding/nih_fpo.png" STYLE="HEIGHT:95px;"/&gt; --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- &lt;img src="images/funding/darpa_fpo.png" STYLE=" HEIGHT:95px;"/&gt; --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- &lt;img src="images/funding/iarpa_fpo.jpg" STYLE="HEIGHT:95px;"/&gt; --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- &lt;img src="images/funding/KAVLI.jpg" STYLE="HEIGHT:95px;"/&gt; --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- &lt;img src="images/funding/schmidt.jpg" STYLE="HEIGHT:95px;"/&gt; --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p3"><span class="s2">background-image: url(images/l_and_v.jpeg)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">.footnote[Questions?]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p3"><span class="s2">name:appendix<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Appendix</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### What are we trying to solve?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">1. Formally define lifelong learning</span></p>
<p class="p3"><span class="s2">2. Design and build machines the lifelong learn</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### What is lifelong Learning?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- A lifelong learning setting is a stream of .ye[potentially changing] tasks. --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">A system lifelong learns when, given a data stream with dynamically changing tasks, the system's<span class="Apple-converted-space">  </span>performance improves by .ye[leveraging prior and future task] data.</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">An .ye[efficient] lifelong learner does so under space/time complexity constraints.</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">Thus, the only way to lifelong learn is by .ye[transferring knowledge across tasks], ideally both .ye[forward] (to improve future task performance) and .ye[backward] (to improve past task performance).</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### A Simple Story</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">As anybody who plays a musical instrument, or a sport, understands, tasks are complex compositions of many sub-tasks.<span class="Apple-converted-space">  </span>This insight is important, because it motivates practicing sub-tasks, which, when improved, yields performance on the original (prior) task of the particular instrument or sport.<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">Artificial intelligence, however, has struggled deeply with sequentially learning how to perform different tasks, something we call "progressive intelligence". Specifically, although many AI solutions exist for transferring knowledge forward to improve new tasks, this typically comes with a cost of forgetting the past to some degree.<span class="Apple-converted-space">  </span>Anti-forgetting is the process of learning new skills or memories that actually enhance past skills or memories, and is the key reason that cross-training works.<span class="Apple-converted-space">  </span>We developed an approach to anti-forgetting called "progressive representation learning", which sequentially learns new representations of data such that performance on both past and future tasks improves.<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### What is the Learning Problem?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">In task $t$, given $n$ new samples,</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">Find $f$ that minimizes the generalization error</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$$f^*\_n = \arg \min\_{f \in \mathcal{F}} \, \mathcal{E}_n(f).$$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### The Transfer Learning Problem</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Given a transfer setting $t = (s, P_0, P_1)$, $n_0$ side information samples, $n_1$ target samples</span></p>
<p class="p3"><span class="s2">- Find $f$ that minimizes generalization error</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>$$f\_{n\_0, n\_1}^* = \arg \min\_{f} \, \mathcal{E}\_{n\_0, n\_1}(f).$$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### CIFAR-10x10 Previous SOTA</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"images/l2m_18mo/progressive_netsc.png"</span><span class="s3"> </span><span class="s4">style</span><span class="s3">=</span><span class="s2">"width:650px;"</span><span class="s1">/&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">Andrei A. Rusu et al. [Progressive Neural Networks](https://arxiv.org/abs/1606.04671), arXiv, 2016.</span></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space">  </span></span></p>
<p class="p5"><span class="s2">&lt;!-- Seungwon Lee, James Stokes, and Eric Eaton. "[Learning Shared Knowledge for Deep Lifelong Learning Using Deconvolutional Networks](https://www.ijcai.org/proceedings/2019/393)." IJCAI, 2019. --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">Lifelong Forests accuracy is worse than .ye[C]NNs with O(10M) parameters, and better than .ye[D]NNs with O(1M) parameters.<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 100%](images/cifar-100-accuracy.svg)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### What is Online Learning?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Let<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- data arrive sequentially in $n$ batches</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- we also observe the prediction of<span class="Apple-converted-space">  </span>experts, collectively in $\Xi$</span></p>
<p class="p3"><span class="s2">- Assume .ye[nothing], $Q\_i \sim P_i \in \mathcal{P}$,<span class="Apple-converted-space">  </span>distribution could be i.i.d., conditionally dependent, or adversarial</span></p>
<p class="p3"><span class="s2">- Define a class of .ye[online] learning algorithms $f$ as a maps</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$$ \mathcal{F}_{O} = \lbrace f : \mathcal{H} \times \color{yellow}{\Xi} \rightarrow \mathcal{H} \rbrace$$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### An Online Learning Task?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Given</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- a online learning setting $( \mathcal{Z}, \mathcal{A}, \mathcal{Q}, \mathcal{P}, \mathcal{C})$, where $\mathcal{C}$ includes that $f \in \mathcal{O}(1) \, \forall n$</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- a risk $R\_i$ at each batch $i$</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- expert advice $\xi\_i$ at each batch $i$</span></p>
<p class="p3"><span class="s2">- Find $f$ that minimizes .ye[regret]</span></p>
<p class="p3"><span class="s2">$$f^* = \arg \min\_{f} \, \mathcal{E}(f, n) = \sum\_{i=1}^n R\_i(f(h\_{i-1}, \xi\_i)). $$<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- - \min\_{h \in \mathcal{H}} \sum\_{i=1}^n R\_i(h).$$ --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Reinforcement Learning?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Let<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- data (states) arrive sequentially in $n$ batches</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- $\mathcal{Z}\_i$ be the space of past states and actions at batch $i$</span></p>
<p class="p3"><span class="s2">- Assume upon taking action $a$, state distribution changes according to some transition matrix transition matrix $[P\_{s, s' \mid a}]$ (for finite $\mathcal{Q}$ and $\mathcal{A}$).</span></p>
<p class="p3"><span class="s2">- Let $\mathcal{H}$ be the space of policies (hypotheses)</span></p>
<p class="p3"><span class="s2">- Define a .ye[reinforcment] learning algorithms $f$ as a sequence</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$$ \mathcal{F}_{R} = \lbrace f\_i : \, \color{yellow}{\mathcal{Z}_i} \times \mathcal{H} \rightarrow \mathcal{H} \rbrace$$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### A Reinforcement Learning Task?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Given</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- reinforcement learning settings $( \mathcal{Z}\_i, \mathcal{A}, \mathcal{Q}, \mathcal{C})\_i$, where<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>- $\mathcal{Q}$ and $\mathcal{A}$ are the state and action spaces, respectively,</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>- $\mathcal{Z}\_i = (\mathcal{Q} \times \mathcal{A})^{i-1}$ is the space of past state-action pairs</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- a discount rate $\gamma$</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- a reward function $\bar{R}$</span></p>
<p class="p3"><span class="s2">- Find $f$ that maximizes .ye[expected reward]</span></p>
<p class="p3"><span class="s2">$$ f^* = \arg \min\_{f} \, \mathcal{E}(f, n) = -\mathbb{E}\left[ \sum\_{i=0}^n \gamma^{n-i} \bar{R}(Q\_i, f(Z\_i, h\_{i-1}))\right] $$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Background<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">3. T. M. Tomita et al. [Sparse<span class="Apple-converted-space">  </span>Projection Oblique Randomer Forests](https://arxiv.org/abs/1506.03410). arXiv, 2018.</span></p>
<p class="p3"><span class="s2">7. J. Browne et al. [Forest Packing: Fast, Parallel Decision Forests](https://arxiv.org/abs/1806.07300). SIAM ICDM, 2018.</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">More info: [https://neurodata.io/sporf/](https://neurodata.io/sporf/)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Do brains do it?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">(brains obviously learn)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">1. Do brains partition feature space?</span></p>
<p class="p3"><span class="s2">2. Is there some kind of "voting" occurring within each part?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Brains partition <span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Feature space = the set of all possible inputs to a brain</span></p>
<p class="p3"><span class="s2">- Partition = only a subset of "nodes" respond to any given input</span></p>
<p class="p3"><span class="s2">- Examples</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>1. visual receptive fields</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>2. place fields / grid cells</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>3. sensory homonculus</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p1"><span class="s1">&lt;</span><span class="s2">br</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"images/rock20/Side-black.gif"</span><span class="s3"> </span><span class="s4">style</span><span class="s3">=</span><span class="s2">"height:230px;"</span><span class="s1">/&gt;</span></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"images/rock20/Front_of_Sensory_Homunculus.gif"</span><span class="s3"> </span><span class="s4">style</span><span class="s3">=</span><span class="s2">"height:230px;"</span><span class="s1">/&gt;</span></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"images/rock20/Rear_of_Sensory_Homunculus.jpg"</span><span class="s3"> </span><span class="s4">style</span><span class="s3">=</span><span class="s2">"height:230px;"</span><span class="s1">/&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Brains vote</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Vote = pattern of responses indicate which stimulus evoked response</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"images/rock20/brody1.jpg"</span><span class="s3"> </span><span class="s4">style</span><span class="s3">=</span><span class="s2">"height:400px;"</span><span class="s3"> </span><span class="s1">/&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Can Humans Backward Transfer?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- "Knowledge and skills from a learner’s first language are used and reinforced, deepened, and expanded upon when a learner is engaged in second language literacy tasks." -- [American Council on the Teaching of Foreign Languages](https://www.actfl.org/guiding-principles/literacy-language-learning)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Proposed Experiments<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Behavioral Experiment</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- Source Task: Delayed Match to Sample (DMS) on colors</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- Target Task A: Delayed Match to Not-Sample<span class="Apple-converted-space">  </span>on colors<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- Target Task B: DMS on orientation<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- Measurements</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- Arc-GFP to identify which neurons could learn<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- Ca2+-YFP to measure neural activity</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- Narp-RFP to identify which neurons actually consolidate</span></p>
<p class="p3"><span class="s2">- Species<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">      </span>- Zebrafish (Engert)</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">      </span>- Mouse (McNaughton and/or Tolias)</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">      </span>- Human (Isik)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Not So Clevr</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"images/not-so-clevr.png"</span><span class="s3"> </span><span class="s4">style</span><span class="s3">=</span><span class="s2">"width:650px"</span><span class="s3"> </span><span class="s1">/&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### RF is more computationally efficient<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p4"><span class="s1">&lt;</span><span class="s6">img</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"images/s-rerf_6plot_times.png"</span><span class="s3"> </span><span class="s4">style</span><span class="s3">=</span><span class="s2">"width:750px;"</span><span class="s1">/&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Scenario Desiderata</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">1. </span><span class="s6">&amp;ge;</span><span class="s2">1 classification<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">1. </span><span class="s6">&amp;ge;</span><span class="s2">1 non-vision<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">1. </span><span class="s6">&amp;ge;</span><span class="s2">1 cross-modal (vision to text)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">I'll propose a few classification domains.</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Vision Task</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- EfficientNet used we different image datasets<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- Each has different number of classes, samples</span></p>
<p class="p3"><span class="s2">- Within dataset images are different sizes / aspect ratios</span></p>
<p class="p3"><span class="s2">- Sequentially train on each dataset</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 50%](images/12-datasets.png)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">Why it is a good scenario:</span></p>
<p class="p3"><span class="s2">1. Images are<span class="Apple-converted-space">  </span>real (different resolutions, scales, # classes)</span></p>
<p class="p3"><span class="s2">2. Many metrics (localization, fine grained objects, texture, scene)</span></p>
<p class="p3"><span class="s2">3. SOTA benchmark results are available<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">4. Much larger than any existing image dataset</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Vision Task</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">.small[</span></p>
<p class="p3"><span class="s2">1. What is application domain</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- machine vision, image classification, object detection, etc.</span></p>
<p class="p3"><span class="s2">2. What are the distributions from which tasks are sampled?</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- 12 different tasks, can sample in arbitrary order to get errorbars on performance<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">3. What is known by agent before deployment? What gets learned?</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>1. Only knowing setting<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>2. pretrained on other image datasets<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>3. convolutions are helpful</span></p>
<p class="p3"><span class="s2">4. What must be selectively remembered across tasks?</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>1. transformers (eg, hidden layers) from previous tasks <span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">5. How does scenario present signal and noise<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>1. many samples per class define signal per class<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">6. What aspects are unique to lifelong learning<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>1. sequential tasks<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">7. Independent and dependent variables?</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>1. Independent: # classes, # samples/class, aspect-ratio/image, image-size/image, object-location/image,<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>2. Dependent variables: forward and backward transfer efficiency</span></p>
<p class="p3"><span class="s2">]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Language Task 1</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- 8,194,317 sentences from wikipedia (downloaded from facebook).<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">- 156 languages</span></p>
<p class="p5"><span class="s2">&lt;!-- - Trained using unsupervised FastText embedding --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- - words, 2-4 char n-grams embedded into 16 dimensions --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- - selected 30 languages --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- - break into batches of 3 "related" languages --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">![:scale 50%](images/30-languages.png)<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">Why it is a good scenario:</span></p>
<p class="p3"><span class="s2">1. Public and real data</span></p>
<p class="p3"><span class="s2">2. Not vision</span></p>
<p class="p3"><span class="s2">3. Many metrics (translation, language identification, grammar correcting, reference adding, etc.)</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Language Task 1</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">.small[</span></p>
<p class="p3"><span class="s2">1. What is application domain</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- natural language processing</span></p>
<p class="p3"><span class="s2">2. What are the distributions from which tasks are sampled?</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- Natural sentences from 156 different languages.</span></p>
<p class="p3"><span class="s2">3. What is known by agent before deployment? What gets learned?</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>1. Only knowing setting<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>2. pretrained on other language datasets<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>3. Word embeddings from existing models</span></p>
<p class="p3"><span class="s2">4. What must be selectively remembered across tasks?</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>1. transformers (eg, hidden layers) from previous languages <span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">5. How does scenario present signal and noise<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>1. many samples per class define signal per language<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">6. What aspects are unique to lifelong learning<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>1. sequential tasks<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">7. Independent and dependent variables?</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>1. Independent: # languages, # sentences/language<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>2. Dependent variables: forward and backward transfer efficiency</span></p>
<p class="p3"><span class="s2">]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Language Task 2</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">.pull-left[</span></p>
<p class="p3"><span class="s2">- Same feature data as above</span></p>
<p class="p3"><span class="s2">- labels now correspond to Microsoft Bing "dominant type"</span></p>
<p class="p5"><span class="s2">&lt;!-- - 10k training and 1k testing entities --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- - 20 classes (each with at least 11k samples) --&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- - 4 classes per task --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">Why is this a good scenario:</span></p>
<p class="p3"><span class="s2">1. Public<span class="Apple-converted-space">  </span>data<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">2. Real application</span></p>
<p class="p3"><span class="s2">3. Not vision</span></p>
<p class="p3"><span class="s2">4. Many metrics (hierarchical classification)<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">.pull-right[</span></p>
<p class="p3"><span class="s2">![:scale 100%](images/bing-dominant-types.png)<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space">    </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### What is Meta-Learning?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Given<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- Environment: $t_i \in \mathcal{T}$ (potentially infinite) set of tasks<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- Measurement space: $\mathcal{Z} \leftarrow (\mathcal{Z},\mathcal{T})$</span></p>
<p class="p3"><span class="s2">- Assume a statistical model:<span class="Apple-converted-space">  </span>$\mathcal{P} = \lbrace P := P\_{Z,T} \otimes P\_{Q,A} \rbrace$</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>-<span class="Apple-converted-space">  </span>$Z\_i | T\_i \sim P\_{Z|T}$,<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- $(T\_1,\ldots, T\_n) \sim P\_T$</span></p>
<p class="p3"><span class="s2">- Define a meta-learning algorithm $f$ to update to provide a hypothesis for a as yet unseen task</span></p>
<p class="p5"><span class="s2">&lt;!-- - Define a lifelong learning algorithm $f$ as a sequence<span class="Apple-converted-space">  </span>--&gt;</span></p>
<p class="p3"><span class="s2">$$ \mathcal{F} = \lbrace f_n :<span class="Apple-converted-space">  </span>(\mathcal{Z} \times \color{yellow}{\mathcal{T}})^n \rightarrow \color{yellow}{\mathcal{H}} \rbrace$$</span></p>
<p class="p3"><span class="s2">- Requires .ye[out of task] capabilities</span></p>
<p class="p3"><span class="s2">- - Define a meta-error $\tilde{\mathcal{E}}$ as a function over all<span class="Apple-converted-space">  </span>performance measures, e.g.<span class="Apple-converted-space">  </span>the mean (or min) generalization error over all potential tasks.<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- - $N_T$ is the number of tasks observed after $n$ samples --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Has $f$ Meta-Learned?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">In meta-learning setting $\mathcal{S}$, given $n$<span class="Apple-converted-space">  </span>samples,<span class="Apple-converted-space">  </span>assuming $P$,<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$f$ .ye[meta-]learns<span class="Apple-converted-space">  </span>when its meta-performance $\mathcal{E}$ improves due to the source data:</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$$f \text{ learns when } \tilde{\mathcal{E}}\_{T+1}(f\_n) </span><span class="s8">&lt;</span><span class="s2"> \, \tilde{\mathcal{E}}\_{T+1}(f\_0).$$</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">Where $\tilde{\mathcal{E}}_{T+1}$ denote the meta-generalization error, such as expected generalization error on some unseen task. <span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- TODO@ronak this doesn't seem quite right. i feel like i need 2 n's, one for the data on all the observed tasks, and one for the data on the as yet unobserved task? --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">#### What is Task-Aware Lifelong Learning?<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Given<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- Environment: $t_i \in \mathcal{T}$ (potentially infinite) set of tasks<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- Measurement space: $\mathcal{Z} \leftarrow (\bigcup_{i \in \mathcal{T}}\mathcal{Z}_i,\mathcal{T})$</span></p>
<p class="p3"><span class="s2">- Assume a statistical model:<span class="Apple-converted-space">  </span>$\mathcal{P} = \lbrace P := P_{Z,T} \otimes P_Q \rbrace$</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>-<span class="Apple-converted-space">  </span>$Z\_i | T\_i \sim P\_{Z|T}$,<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- $(T\_1,\ldots, T\_n) \sim P\_T$</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- data arrive sequentially in batches</span></p>
<p class="p3"><span class="s2">- Define a task-aware lifelong learning algorithm $f$ to update existing hypotheses on the basis of a batch of $m$ new samples</span></p>
<p class="p5"><span class="s2">&lt;!-- - Define a lifelong learning algorithm $f$ as a sequence<span class="Apple-converted-space">  </span>--&gt;</span></p>
<p class="p3"><span class="s2">$$ \mathcal{F} = \lbrace f : \mathcal{H}<span class="Apple-converted-space">  </span>\times \mathcal{Z}^m \rightarrow \mathcal{H} \rbrace$$</span></p>
<p class="p5"><span class="s2">&lt;!-- - Requires .ye[out of task] capabilities <span class="Apple-converted-space">  </span>--&gt;</span></p>
<p class="p5"><span class="s2">&lt;!-- - $N_T$ is the number of tasks observed after $n$ samples --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">#### A Task-Aware Lifelong Learning Task</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">In task-aware lifelong<span class="Apple-converted-space">  </span>setting $\mathcal{S}$, given $n$ samples, <span class="Apple-converted-space">  </span>assuming $P$,<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$f$ weakly lifelong learns when its<span class="Apple-converted-space">  </span>performances $\lbrace \mathcal{E}_t \rbrace$ improve due to other task's data on average:</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- $$ \mathbb{E} \left[ \sum\_{t \in [T\_n]} \mathcal{E}\_t(f\_n )<span class="Apple-converted-space">  </span>\right]<span class="Apple-converted-space">  </span>&lt;<span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space"> </span>\mathbb{E} \sum\_{t \in [T\_n]} \mathcal{E}\_t(f\_n^t),$$<span class="Apple-converted-space">  </span>--&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$$<span class="Apple-converted-space">  </span>\sum\_{t \in \mathcal{T}} n\_t \mathcal{E}\_t(f\_n ) <span class="Apple-converted-space">  </span></span><span class="s8">&lt;</span><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space"> </span>\sum\_{t \in \mathcal{T}} n\_t \mathcal{E}\_t(f\_n^t) ,$$<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s3"><span class="Apple-converted-space"> </span></span><span class="s2">&lt;!-- $$ \sum\_{i \in [n]} \mathcal{E}\_{t\_i}(f\_i )<span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space"> </span>&lt;<span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>\sum\_{i \in [n]} \mathcal{E}\_{t\_i}(f\_i^{t_i}),$$<span class="Apple-converted-space">  </span>--&gt;</span></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p1"><span class="s1">&lt;</span><span class="s2">br</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$f$ strongly lifelong learns when its performances $\lbrace \mathcal{E}_t \rbrace$ improve due to other task's data for each task:</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$$ \mathcal{E}\_t(f_n) </span><span class="s8">&lt;</span><span class="s2"> \, \mathcal{E}\_t(f_n^t) \quad \forall t \in \mathcal{T}.$$<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">#### What is Task-Oblivious Lifelong Learning?<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Given</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space"> </span>- Environment: $t_i \in \color{yellow}{\mathcal{T}}$<span class="Apple-converted-space">  </span>(.ye[potentially infinite]) set of tasks</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space"> </span>- .ye[Side information]: $W_i \in \mathcal{W}$ such as expert advise, or reinforcement learning structure</span></p>
<p class="p5"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s2">&lt;!-- - each batch may be associated with a new task --&gt;</span></p>
<p class="p5"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s2">&lt;!-- - the sequence of tasks is called the .ye[syllabus] --&gt;</span></p>
<p class="p5"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s2">&lt;!-- - there is potential for<span class="Apple-converted-space">  </span>$\Xi$-valued side information<span class="Apple-converted-space">  </span>--&gt;</span></p>
<p class="p3"><span class="s2">- Assume</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- data arrive .ye[sequentially] in batches (potentially of size 1)</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- .ye[no<span class="Apple-converted-space">  </span>structure] to $P$, ie, could be iid, adversarial, etc.</span></p>
<p class="p3"><span class="s2">- Define a task-oblivious lifelong learning algorithm $f$ to update existing hypotheses on the basis of a batch of $m$ new samples</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$$\mathcal{F} = \lbrace<span class="Apple-converted-space">  </span>f\_m : \color{yellow}{\mathcal{H}}<span class="Apple-converted-space">  </span>\times ({\mathcal{Z}} \times \mathcal{W})^m \rightarrow \mathcal{H} \rbrace$$<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">- Note:</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>-<span class="Apple-converted-space">  </span>$f$ is oblivious to whether the setting has changed<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- $n_t$ is the number of samples for task $t$</span></p>
<p class="p5"><span class="s2">&lt;!-- - $m=1$ is a special case --&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">#### A Task-Oblivious Lifelong Learning Task</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">In task-oblivious lifelong setting $\mathcal{S}$, given $n$ samples,<span class="Apple-converted-space">  </span>assuming $P$,<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$f$ .ye[weakly lifelong] learns when its<span class="Apple-converted-space">  </span>performances $\lbrace \mathcal{E}_t \rbrace$ improve due to other task's data .ye[on average]:</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s2">&lt;!-- $$ \mathbb{E} \left[ \sum\_{t \in [T\_n]} \mathcal{E}\_t(f\_n )<span class="Apple-converted-space">  </span>\right]<span class="Apple-converted-space">  </span>&lt;<span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space"> </span>\mathbb{E} \sum\_{t \in [T\_n]} \mathcal{E}\_t(f\_n^t),$$<span class="Apple-converted-space">  </span>--&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$$<span class="Apple-converted-space">  </span>\sum\_{t \in \mathcal{T}} n\_t \mathcal{E}\_t(f\_n ) <span class="Apple-converted-space">  </span></span><span class="s8">&lt;</span><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space"> </span>\sum\_{t \in \mathcal{T}} n\_t \mathcal{E}\_t(f\_n^t) ,$$<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s3"><span class="Apple-converted-space"> </span></span><span class="s2">&lt;!-- $$ \sum\_{i \in [n]} \mathcal{E}\_{t\_i}(f\_i )<span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space"> </span>&lt;<span class="Apple-converted-space"> </span></span></p>
<p class="p5"><span class="s2"><span class="Apple-converted-space">  </span>\sum\_{i \in [n]} \mathcal{E}\_{t\_i}(f\_i^{t_i}),$$<span class="Apple-converted-space">  </span>--&gt;</span></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p1"><span class="s1">&lt;</span><span class="s2">br</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$f$ .ye[strongly lifelong] learns when its performances $\lbrace \mathcal{E}_t \rbrace$ improve due to other task's data .ye[for each task]:</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">$$ \mathcal{E}\_t(f_n) </span><span class="s8">&lt;</span><span class="s2"> \, \mathcal{E}\_t(f_n^t) \quad \forall t \in \mathcal{T}.$$<span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### What is a Meta-Task?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### What is Streaming Learning?</span></p>
<p class="p3"><span class="s2">### Has $f$ Streaming Learned?</span></p>
<p class="p3"><span class="s2">### What is a streaming task?</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">---</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">### Language Task 2</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2">.small[</span></p>
<p class="p3"><span class="s2">1. What is application domain</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- natural language processing</span></p>
<p class="p3"><span class="s2">2. What are the distributions from which tasks are sampled?</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>- Many words from each Bing dominant type</span></p>
<p class="p3"><span class="s2">3. What is known by agent before deployment? What gets learned?</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>1. Only knowing setting<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>2. pretrained on other data<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>3. Word embeddings from existing models</span></p>
<p class="p3"><span class="s2">4. What must be selectively remembered across tasks?</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>1. transformers (eg, hidden layers) from other types <span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">5. How does scenario present signal and noise<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>1. many samples per class define signal per dominant type<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">6. What aspects are unique to lifelong learning<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>1. sequential tasks<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2">7. Independent and dependent variables?</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>1. Independent: # types, # terms/type<span class="Apple-converted-space"> </span></span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">  </span>2. Dependent variables: forward and backward transfer efficiency</span></p>
<p class="p3"><span class="s2">]</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s2"><span class="Apple-converted-space"> </span></span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">textarea</span><span class="s1">&gt;</span></p>
<p class="p5"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s2">&lt;!-- &lt;script src="https://gnab.github.io/remark/downloads/remark-latest.min.js"&gt;&lt;/script&gt; --&gt;</span></p>
<p class="p5"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s2">&lt;!-- &lt;script src="remark-latest.min.js"&gt;&lt;/script&gt; --&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">script</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"remark-latest.min.js"</span><span class="s1">&gt;&lt;/</span><span class="s6">script</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">script</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.js"</span><span class="s1">&gt;&lt;/</span><span class="s6">script</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">script</span><span class="s3"> </span><span class="s4">src</span><span class="s3">=</span><span class="s2">"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/contrib/auto-render.min.js"</span><span class="s1">&gt;&lt;/</span><span class="s6">script</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">link</span><span class="s3"> </span><span class="s4">rel</span><span class="s3">=</span><span class="s2">"stylesheet"</span><span class="s3"> </span><span class="s4">href</span><span class="s3">=</span><span class="s2">"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"</span><span class="s1">&gt;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;</span><span class="s6">script</span><span class="s3"> </span><span class="s4">type</span><span class="s3">=</span><span class="s2">"text/javascript"</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span></span><span class="s6">var</span><span class="s2"> </span><span class="s4">options</span><span class="s2"> = {};</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span></span><span class="s6">var</span><span class="s2"> </span><span class="s4">renderMath</span><span class="s2"> = </span><span class="s6">function</span><span class="s2"> () {</span></p>
<p class="p7"><span class="s3"><span class="Apple-converted-space">      </span></span><span class="s2">renderMathInElement</span><span class="s3">(</span><span class="s4">document</span><span class="s3">.</span><span class="s4">body</span><span class="s3">);</span></p>
<p class="p5"><span class="s3"><span class="Apple-converted-space">      </span></span><span class="s2">// or if you want to use $...$ for math,</span></p>
<p class="p7"><span class="s3"><span class="Apple-converted-space">      </span></span><span class="s2">renderMathInElement</span><span class="s3">(</span><span class="s4">document</span><span class="s3">.</span><span class="s4">body</span><span class="s3">, {</span></p>
<p class="p5"><span class="s3"><span class="Apple-converted-space">        </span></span><span class="s4">delimiters:</span><span class="s3"> [ </span><span class="s2">// mind the order of delimiters(!?)</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">          </span>{ </span><span class="s4">left:</span><span class="s2"> </span><span class="s7">"$$"</span><span class="s2">, </span><span class="s4">right:</span><span class="s2"> </span><span class="s7">"$$"</span><span class="s2">, </span><span class="s4">display:</span><span class="s2"> </span><span class="s6">true</span><span class="s2"> },</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">          </span>{ </span><span class="s4">left:</span><span class="s2"> </span><span class="s7">"$"</span><span class="s2">, </span><span class="s4">right:</span><span class="s2"> </span><span class="s7">"$"</span><span class="s2">, </span><span class="s4">display:</span><span class="s2"> </span><span class="s6">false</span><span class="s2"> },</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">          </span>{ </span><span class="s4">left:</span><span class="s2"> </span><span class="s7">"</span><span class="s9">\\</span><span class="s7">["</span><span class="s2">, </span><span class="s4">right:</span><span class="s2"> </span><span class="s7">"</span><span class="s9">\\</span><span class="s7">]"</span><span class="s2">, </span><span class="s4">display:</span><span class="s2"> </span><span class="s6">true</span><span class="s2"> },</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">          </span>{ </span><span class="s4">left:</span><span class="s2"> </span><span class="s7">"</span><span class="s9">\\</span><span class="s7">("</span><span class="s2">, </span><span class="s4">right:</span><span class="s2"> </span><span class="s7">"</span><span class="s9">\\</span><span class="s7">)"</span><span class="s2">, </span><span class="s4">display:</span><span class="s2"> </span><span class="s6">false</span><span class="s2"> },</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">        </span>]</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">      </span>});</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>}</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p8"><span class="s3"><span class="Apple-converted-space">    </span></span><span class="s2">remark</span><span class="s3">.</span><span class="s2">macros</span><span class="s3">.</span><span class="s10">scale</span><span class="s3"> = </span><span class="s6">function</span><span class="s3"> (</span><span class="s2">percentage</span><span class="s3">) {</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">      </span></span><span class="s6">var</span><span class="s2"> </span><span class="s4">url</span><span class="s2"> = </span><span class="s6">this</span><span class="s2">;</span></p>
<p class="p4"><span class="s3"><span class="Apple-converted-space">      </span></span><span class="s11">return</span><span class="s3"> </span><span class="s2">'&lt;img src="'</span><span class="s3"> + </span><span class="s4">url</span><span class="s3"> + </span><span class="s2">'" style="width: '</span><span class="s3"> + </span><span class="s4">percentage</span><span class="s3"> + </span><span class="s2">'" /&gt;'</span><span class="s3">;</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span>};</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p5"><span class="s3"><span class="Apple-converted-space">    </span></span><span class="s2">// var slideshow = remark.create({</span></p>
<p class="p5"><span class="s3"><span class="Apple-converted-space">    </span></span><span class="s2">// Set the slideshow display ratio</span></p>
<p class="p5"><span class="s3"><span class="Apple-converted-space">    </span></span><span class="s2">// Default: '4:3'</span></p>
<p class="p5"><span class="s3"><span class="Apple-converted-space">    </span></span><span class="s2">// Alternatives: '16:9', ...</span></p>
<p class="p3"><span class="s2"><span class="Apple-converted-space">    </span></span><span class="s12">// {</span></p>
<p class="p5"><span class="s3"><span class="Apple-converted-space">    </span></span><span class="s2">// ratio: '16:9',</span></p>
<p class="p5"><span class="s3"><span class="Apple-converted-space">    </span></span><span class="s2">// });</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p8"><span class="s3"><span class="Apple-converted-space">    </span></span><span class="s6">var</span><span class="s3"> </span><span class="s2">slideshow</span><span class="s3"> = </span><span class="s2">remark</span><span class="s3">.</span><span class="s10">create</span><span class="s3">(</span><span class="s2">options</span><span class="s3">, </span><span class="s2">renderMath</span><span class="s3">);</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p1"><span class="s3"><span class="Apple-converted-space">  </span></span><span class="s1">&lt;/</span><span class="s2">script</span><span class="s1">&gt;</span></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">body</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
<p class="p1"><span class="s1">&lt;/</span><span class="s2">html</span><span class="s1">&gt;</span></p>
<p class="p2"><span class="s5"></span><br></p>
</body>
</html>
