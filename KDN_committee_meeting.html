<!DOCTYPE html>
<html>

  
<head>
  <title>Learning</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <link rel="stylesheet" href="fonts/quadon/quadon.css">
  <link rel="stylesheet" href="fonts/gentona/gentona.css">
  <link rel="stylesheet" href="slides_style_i.css">
  <script type="text/javascript" src="assets/plotly/plotly-latest.min.js"></script>

</head>

<body>
  <textarea id="source">

<div>
  

<img src=
  "images/jhu.png" 
    alt="jhu logo" 
    align="right"
    width = "240"
    height= "125">
</div>
<br>
<br>
<br>
##Deep Discriminative to Kernel Density Networks for In- and Out-of-distribution Calibrated Inference

Jayanta Dey, Will LeVine, Haoyin Xu, Ashwin De Silva <br>
PI: Joshua T. Vogelstein, [JHU](https://www.jhu.edu/)

<center>
![:scale 30%](images/neurodata_blue.png)
<br>
</center>
---
## Charles Blondin Illustration

<center>
![:scale 100%](images/blondin.png)
</center>

---
## Overcondient on Wrongly Classified Labels

<center>
  ![:scale 100%](images/overconfident.png)
  </center>

---
## What is calibration?

We say a model is calibrated if 
<br>
<center>
  $\hat{P}(y|x) = P(y|x)$
</center>

Indirect way of measuring it 
<center>
$P(Y=y|\hat{P} = p) = p$
</center>
---
## Existing methods

### .w[In-distribution Calibration]
- Platt Scaling
- Isotonic Regression
- Temperature Scaling
- Ensemble Methods

### .w[OOD Calibration]
- Discriminative 
- Generative 
---
## What are we solving?
- Address in-distribution (ID) and out-of-distribution (OOD) calibration as a continuum.

- Address the confidence calibration problems for both ReLU-nets and random forests from a common ground.

- Provide guarantees for asymptotic performance.

- Do unsupervised OOD calibration.

---
## Problem Formulation
- Consider a supervised learning problem with $\mathrm{IID}$ training samples $\{ (\mathbf{x}\_i, y\_i)\}\_{i=1}^n$ 
- $(X, Y) \sim P\_{X, Y}$, where $X \sim P\_X$ is a $\mathcal{X} \subseteq \mathbb{R}^d$ valued input and $Y \sim P\_Y$ is a $\mathcal{Y} = \{1, \cdots, K\}$ valued class label. 
- We define, $\mathcal{S}$ as high density region of $P\_{X}$. 

We want to estimate $g\_y(\mathbf{x})$ such that:
<br>
$$g\_y(\mathbf{x}) =  P\_{Y|X}(y|\mathbf{x}), ~\text{if} ~\mathbf{x} \in \mathcal{S}$$
$$      = P\_Y(y), ~\text{if} ~\mathbf{x} \notin \mathcal{S}$$
---
## How Deep Discriminative Networks Partition 
<br>
<center>
  ![:scale 100%](images/polytopes.png)
</center>
---
## Traditional Approach
 The set of learned polytopes $(1,2, \cdots, p)$
$$\hat{f}\_y(\mathbf{x}) = \sum\_{r=1}^{p} (a\_r^T \mathbf{x} + b\_r)I(\mathbf{x} \in Q\_r)$$

---
## Our Approach
### .w[We replace the affine activations:]
$$\hat{f}\_y(\mathbf{x}) = \frac{1}{n\_y}\sum\_{r \in \mathcal{P}}n\_{ry} G(\mathbf{x}, \hat{\mu}\_r, \hat{\Sigma}\_r)I(r = r^*\_{\mathbf{x}}) + \frac{b}{n}$$

$$\text{where}, r^*\_{\mathbf{x}} = argmin~\_r ~|\mu\_r - \mathbf{x}|$$

#### .w[We estimate $g\_y(x)$ as:]
$$\hat{g}\_y(\mathbf{x}) = \frac{\hat{f}\_y(\mathbf{x}) \hat{P}\_Y(y)}{\sum\_{k=1}^{K} \hat{f}\_k(\mathbf{x}) \hat{P}\_Y(k)}$$
---
## Desiderata
### .w[Asymptotic Performance]
- As $n \to \infty$:
$$\max\_{y \in \mathcal{Y}} \sup_{\mathbf{x} \in \mathbb{R}^d} |g\_y(\mathbf{x}) - \hat{g}\_y(\mathbf{x})| \to 0$$

### .w[Finite Sample Performance]
- Used various simulation, $46$ Openml-cc18 datasets and vision datasets.
---
## Conditions for Achieving Goal \#1

- The center of the kernel can be any point $z_r$ within the polytope $Q_r$ as $n \to \infty$.
- The kernel bandwidth along any dimension $\sigma_{r}$ is any positive number always bounded by the polytope bandwidth $h_n$ as $n \to \infty$, i.e., $\sigma_r = C_r h_n$, where $0<C_r\leq 1$.

Conditions on polytopes:
- The average polytope bandwidth $h_n \to 0$ as $n \to \infty$
- $n$ grows faster than the shrinkage of $h_n$, i.e., $n\cdot h_n \to \infty$ as $h_n \to 0$ in probability
---
## Gaussian Parameters Estimation

$$\hat{\mu}\_r = \frac{1}{n\_r} \sum\_{i=1}^{n} \mathbf{x}\_i I(\mathbf{x}\_i \in Q\_r)$$

<br>
<br>
$$\hat{\Sigma}\_r = \frac{ \sum\_{i=1}^n I(\mathbf{x}\_i \in Q\_r) (\mathbf{x}\_i - \hat{\mu}\_r)  (\mathbf{x}\_i - \hat{\mu}\_r)^\top +\lambda I\_d}{ \sum\_{i=1}^n I(\mathbf{x}\_i \in Q\_r)}$$
---
## Sample Size Ratio Estimation

### .w[Randm Forest Kernel:]
For any $x\_i \in Q\_r$ and $x\_j \in Q\_s$,

$$\mathcal{K}(r,s) = \frac{t\_{rs}}{T}$$

---
## Sample Size Ratio Estimation (Continued)

### .w[Deep-net Kernel:]

<center>
  ![:scale 40%](images/weight.png)
</center>

$$\mathcal{K}(r,s) = \frac{\sum\_{z=1}^N I(a\_z^r = a\_z^s)}{N}$$

---
## Sample Size Ratio Estimation (Continued)

$$\hat{\left(\frac{n\_{ry}}{n\_y}\right)} = \frac{\tilde{w}\_{ry}}{\tilde{w}\_y} = \frac{\tilde{w}\_{ry}}{\sum\_{r\in \mathcal{P}} \tilde{w}\_{ry}} = \frac{\sum\_{s \in \mathcal{P}} \sum\_{i=1}^n w\_{rs} I(\mathbf{x}\_i \in Q\_s) I(y\_i = y) }{
  \sum\_{r\in \mathcal{P}} \sum\_{s\in \mathcal{P}} \sum\_{i=1}^n w\_{rs} I(\mathbf{x}\_i \in Q\_s) I(y\_i = y)}$$


  <br>
  <br>
  <br>

$$w\_{rs} = \mathcal{K}(r,s)^{k \log n}$$
---
## Nearest Polytope Estimation

### .w[Geodesic Distance:]
$$d(r,s) = -\mathcal{K}(r,s) + \frac{1}{2} (\mathcal{K}(r,r) + \mathcal{K}(s,s)) = 1 - \mathcal{K}(r,s)$$
---
## Acheiving Goal \#2: Simulation Experiments
<center>
  ![:scale 80%](images/simulations.png)
</center>
---
## Simulation Experiments (Continued)
<center>
  ![:scale 60%](images/simulation_res.png)
</center>
---
## Acheiving Goal \#2: Benchmark Data Study 
<center>
  ![:scale 70%](images/openml_summary.png)
</center>
---
## Conclusions
- Thinking in terms of polytopes is a good way to solve calibration problem in a unified way.
- We measure similarity between polytopes using forest or neural kernel and then measure local similarity using a Gaussian kernel.
---
### Acknowledgements



<!-- <div class="small-container">
  <img src="faces/ebridge.jpg"/>
  <div class="centered">Eric Bridgeford</div>
</div>

<div class="small-container">
  <img src="faces/pedigo.jpg"/>
  <div class="centered">Ben Pedigo</div>
</div>

<div class="small-container">
  <img src="faces/jaewon.jpg"/>
  <div class="centered">Jaewon Chung</div>
</div> -->
<div class="small-container">
  <img src="faces/jovo.png"/>
  <div class="centered">Carey Priebe</div>
</div>

<div class="small-container">
  <img src="faces/cep.png"/>
  <div class="centered">Carey Priebe</div>
</div>

<div class="small-container">
  <img src="faces/hao.jpg"/>
  <div class="centered">Hao</div>
</div>

<div class="small-container">
  <img src="faces/ashwin.png"/>
  <div class="centered">Ashwin De Silva</div>
</div>

<div class="small-container">
  <img src="faces/will.jpg"/>
  <div class="centered">Will LeVine</div>
</div>

<!-- <div class="small-container">
  <img src="faces/mim.jpg"/>
  <div class="centered">Michael Miller</div>
</div>

<div class="small-container">
  <img src="faces/dtward.jpg"/>
  <div class="centered">Daniel Tward</div>
</div> -->


<!-- <div class="small-container">
  <img src="faces/vikram.jpg"/>
  <div class="centered">Vikram Chandrashekhar</div>
</div>


<div class="small-container">
  <img src="faces/drishti.jpg"/>
  <div class="centered">Drishti Mannan</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/kwame.jpg"/>
  <div class="centered">Kwame Kutten</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/perlman.jpg"/>
  <div class="centered">Eric Perlman</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/bcaffo.jpg"/>
  <div class="centered">Brian Caffo</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/minh.jpg"/>
  <div class="centered">Minh Tang</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/avanti.jpg"/>
  <div class="centered">Avanti Athreya</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/vince.jpg"/>
  <div class="centered">Vince Lyzinski</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/dpmcsuss.jpg"/>
  <div class="centered">Daniel Sussman</div>
</div> -->


<!-- <div class="small-container">
  <img src="faces/shangsi.jpg"/>
  <div class="centered">Shangsi Wang</div>
</div> -->

<div class="small-container">
  <img src="faces/tyler.jpg"/>
  <div class="centered">Tyler Tomita</div>
</div>

<div class="small-container">
  <img src="faces/alig.jpg"/>
  <div class="centered">Ali Geisa</div>
</div>
<!-- <div class="small-container">
  <img src="faces/gkiar.jpg"/>
  <div class="centered">Greg Kiar</div>
</div> -->

 

<!-- </div><span style="font-size:200%; color:red;">&hearts;, &#129409;, &#128106;, &#127758;, &#127756;</span> -->

---
## Questions? 
---

</textarea>
  <!-- <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js"></script> -->
  <!-- <script src="remark-latest.min.js"></script> -->
  <script src="remark-latest.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
  <script type="text/javascript">

    var options = {};
    var renderMath = function () {
      renderMathInElement(document.body);
      // or if you want to use $...$ for math,
      renderMathInElement(document.body, {
        delimiters: [ // mind the order of delimiters(!?)
          { left: "$$", right: "$$", display: true },
          { left: "$", right: "$", display: false },
          { left: "\\[", right: "\\]", display: true },
          { left: "\\(", right: "\\)", display: false },
        ]
      });
    }

    remark.macros.scale = function (percentage) {
      var url = this;
      return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };

    // var slideshow = remark.create({
    // Set the slideshow display ratio
    // Default: '4:3'
    // Alternatives: '16:9', ...
    // {
    // ratio: '16:9',
    // });
    
    var slideshow = remark.create(options, renderMath);

  
  </script>
</body>

</html>
